# Three Parameters {#sec-three-parameters}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(gt)
library(gtsummary)
library(patchwork)
```

<!-- Is the chapter too long? Should I just have a single question for each section? Should I cut out one of the sections entirely? -->

<!-- Standardize years. Data is from 2012. Questions are about 2024. -->

<!-- Remove sampling & assignment mechanism -->

<!-- Come up with common ways in which a lack of representativeness causes problems. This is a list of things that we can check each time when we are interrogating a data science problem. Key issue is the correlation, if anything, between the "assignment mechanism" or "sampling mechanism" and other variables. Discuss sampling mechanism in all cases except att_end ~ treatment. For that, use "assignment mechanism." -->

<!-- Make residual plots consistent in this chapter! Also, we should ensure that the width of 10 units is the same in each plot. Or does that already happen? Want the intuition that, the more predictive the model, the smaller the remaining variation in the residuals.  -->

<!-- Whenever we have a predictive model, use language like "when comparing." Avoid even the hint of a causal claim. -->

Models have parameters. In @sec-sampling we created a model with a single parameter $\rho$, the proportion of red beads in an urn. In @sec-two-parameters, we used models with two parameters: $\mu$ (the average height in the population, generically known as a model "intercept") and $\sigma$ (the variation in height in the population). Here --- can you guess where this is going? --- we will build models with three parameters: $\sigma$ (which serves the same role throughout the book) and two "coefficients." In models which relate a predictor to the outcome, those two parameters will be labeled $\beta_0$ and $\beta_1$. All this notation is confusing, not least because different academic fields use inconsistent schemes. Follow the Cardinal Virtues and tackle your problem step by step.

Perhaps more importantly, a focus on parameters is less relevant now than it was decades ago, when computational limitations made answering our actual questions harder. *Parameters are imaginary.* They don't exist. They, in general, are not the answer to a real world question. They are tools, along with the models of which they are a part, we use to answer questions.

Packages:

```{r}
#| message: false
#| code-fold: false
library(primer.data)
library(tidyverse)
library(brms)
library(tidybayes)
```

We are concerned with these three packages because we are exploring the `trains` dataset which exists within the **primer.data** package. We use the **brms** package to build Bayesian models. ("brms" stands for **B**ayesian **r**egression **m**odel**s**.) The **tidybayes** packages makes working the fitted models easierr. As usual, we use the **tidyverse** package.

In this chapter, we are going to ask a series of questions involving train commuters' ages, party affiliations, incomes, and political ideology, as well as the causal effect of exposure to Spanish-speakers on their attitude toward immigration. These questions will pertain to all train commuters in the US today. For a refresher on this data, refer to @sec-rubin-causal-model.



## income \~ age

So far, we have only created models in which the predictor variable is discrete, with two possible values. `party` is either "Democrat" or "Republican". `treatment` is either "Treated" or "Control". Often times, however, the predictor variable will be continuous. We can answer these and similar questions by creating a model that uses party affiliation to predict age. Let's follow the four Cardinal Virtue: Wisdom, Justice, Courage and Temperance. Fortunately, the exact same approach works in this case. Consider:

*What would you expect the income to be for a 40-year old?*

### Wisdom

```{r}
#| echo: false
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of "validity," as to whether or not we can (reasonably!) assume that the two come from the same *population*.

#### Preceptor Table

First, do we need a **causal** or **predictive** model? In our question above we have a predictive model as we are only considered with one outcome: a person's income based on their age. Remember the motto: *No causation without manipulation*.

Second, what is the outcome? A person's income would be our **outcome**. The individual that asked this question did not tell us where or when we are looking at these questions. Do we need to look at the year 2022, 2020, 2018? Do we need to look in Florida, Connecticut, Maryland? It is vital to understand where and when that we are answering these questions because otherwise our data could be not accurately applied to the whole country, nation, or specific state. Therefore, by diving deeper within these questions we are able to accurately answer our question with the population that it refers to. For the questions that we have at hand we will be talking about all the adults in July 1, 2012 at the location of the United States which will allow us to answer the questions more accurately.

Third, what are the **units**? Our units for this scenario would be dollars as we are concerned with dollars at the station for 40-year olds.

Fourth, what are the **covariates**? In this case, a person's age is the only covariate. The Preceptor Table must include any covariates mentioned in the question.

Fifth, do we have a **treatment**? No. We will be using a predictive model as discussed above and treatments will only apply to situations with causal models. A treatment is just a covariate which we can, at least in theory, manipulate, thereby creating other potential outcomes.

Let's look at our refined question to create our Preceptor Table:

*What would you expect the income to be for a 40-year old in the United States in 2012?*

Our Preceptor Table:

```{r}
#| echo: false
tibble(ID = c("1", "2", "...", "10", "11", "...", "N"),
       income = c("150000", "50000", "...", "65000", "35000", "...", "78000"),
       age = c("31", "58", "...", "67", "23", "...", "44"))|>
  
  gt() |>
  tab_header(title = "Preceptor Table") |> 
  cols_label(ID = md("ID"),
             income = md("Income"),
             age = md("Age")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(income)) |>
  tab_spanner(label = "Covariate", columns = c(age))
```

Recall: a Preceptor Table is table with rows and columns such that, if there is no missing data, all our questions are easy to answer. To answer questions — like “What would you expect the income to be for a 40-year old?” — we need a row for every individual at the station.

When we take a look at the questions we discover a flaw exists. The individual that asked this question did not tell us where or when we are looking at these questions. Do we need to look at the year 2022, 2020, 2018? Do we need to look in Florida, Connecticut, Maryland? It is vital to understand where and when that we are answering these questions because otherwise our data could be not accurately applied to the whole country, nation, or specific state. Therefore, by diving deeper within these questions we are able to accurately answer our question with the population that it refers to. For the questions that we have at hand we will be using the year 2024 and the location of Chicago, Illinois which will allow us to answer the questions more accurately. 

#### Data Analysis

Recall the discussion from @sec-rubin-causal-model. @enos2014 randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. The data that we want to analyze consists of the age and party of each individual on these train platforms based on our variables in our Preceptor Table. These reactions were measured through changes in answers to three survey questions.

```{r}
trains |>
  select(age, income)
```

As specified in the Preceptor Table we only care about `age` is the age of the respondent and `income` is the income of the respondent. The `age` and `income` are both numbers either as a double or integer which plays a role in making up an individual's identity.

```{r}
trains |>
  select(age, income) |>
  summary()
```

`summary()` shows us what the different values of `age` and `income` are because it is a factor. The range for `age` seems reasonable. Recall that participants were asked three questions about immigration issues, each of which allowed for an answer indicated strength of agreement on a scale form 1 to 5, with higher values indicating more agreement with conservative viewpoints.

```{r}
trains |>
  ggplot(aes(x = age, y = income)) +
  geom_point() +
  labs(x = "Age",
       y = "Income") +
  scale_y_continuous(labels = scales::dollar_format()) +
  theme_classic()
```

We can never know the true average income of the population. But we can calculate a distribution for each parameter. As we can see with the plot above, the older individuals tend to be have more of an income. When compared with the younger individuals, the older individuals tend to exhibit a greater income because of the longer life and possibilities they have.

#### Validity

Recall that validity refers to whether or not we can consider the columns `age` and `income` to have the same meaning in our data set of 2012 Boston train commuters and in our Preceptor Table. While `age` doesn't really change meaning over time, `income` can be impacted by inflation. After all, \$100,000 in 2012 doesn't have the same worth as \$100,000 now due to inflation and now would have less purchasing power. This would result in income being underestimated within our model. However, since there hasn't been drastic inflation that dramatically changed the buying power, we will consider income to be valid. If there had been like 300% inflation, however, our conclusion would probably be different.

Now the **assumption of validity** may not hold due to the possibility of how our age data was collected. For example, the data has been collected from filling out surveys about age without checking a driver's license. We don't know whether our age is accurate enough when compared to the driver's license of an individual. 

Another possible scenario that could prove the **assumption of validity** wrong could be the privacy of each station. The privacy of a train station differs from filling out surveys at home. The key difference that exists between the Preceptor Table and the data can exist from the difference in privacy that we have between the two sets of data. 

While the assumption can prove validity to be wrong, overall, the assumption of validity seems reasonable enough. `age` and `income` are similar enough between to the underlying concepts in the Preceptor Table and the data that we can "stack" them on top of each other. We can assume that both are drawn from the same population.

### Justice

```{r}
#| echo: false
#| fig.cap: Justice
knitr::include_graphics("other/images/Justice.jpg")
```

Once again, in **Justice**, we must consider the Population Table, stability, representativeness, unconfoundedness and the mathematical structure of the data generating mechanism (DGM).

#### Population Table

With our **assumption of validity** proving to be reasonably accurate, we can now create our Population Table. We can make a Population Table with our Preceptor Table and other data based on our assumptions that we have made.

```{r}
#| echo: false
tibble(source = c("...", "Data", "Data", "...", 
                  "...", "Preceptor Table", "Preceptor Table", "..."),
       income = c("...", "150000", "50000", "...",
                 "...", "...", "...", "..."),
       city = c("...", "Boston, MA", "Boston, MA", "...", 
                "...", "Wilmington, DE", "Atlanta, GA", "..."),
       year = c("...", "2012", "2012", "...", 
                "...", "2024", "2024", "..."),
       age = c("...", "43", "52", "...", 
               "...", "...", "...", "...")) |>
  
  gt() |>
  tab_header(title = "Population Table") |> 
  cols_label(source = md("Source"),
             income = md("Income"),
             city = md("City"),
             year = md("Year"),
             age = md("Age")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(income)) |>
  tab_spanner(label = "Covariates", columns = c(age, year, city))
  
```

Our year within the Population table is an example of the moment in time.

The Population Table includes commuters from our Preceptor Table with the information that we would ideally have to answer the questions and those from the data we have that is specific to Boston, MA. Additionally, the Population Table also includes the groups of people within the population from which both the data we have and the Preceptor table is drawn from that we don't have.

#### Stability

Consider the **stability** of our model and if we believe that our relationship between age and income has changed between 2012 and now. Once again, let's consider inflation and how that could impact income. If incomes were to increase at the rate of inflation, then the income distribution would be different than that of 2012. However, wages don't tend to change as quickly as inflation does, so they likely did not change significantly and we can consider this model to be stable.

#### Representativeness

**Representativeness** has to do with how well our sample represents the larger population we are interested in generalizing to. When we deal with representativeness we deal with two potential problems: Is our data representative of the population? Is our Preceptor Table representative of the population? For example: Can we assume that Boston train commuters are perfectly representative of Chicago train commuters AND those who were approached to respond to the survey? The location in which people commute from can change the way that we interpret their attitude regarding the treatment. For the predictive question, representativeness may not apply as the attitude regarding each individual in Boston might differ from the US as they don't refer to the same broader population making it not representative of the entire population. When we deal with representativeness we deal with two primary concerns: the generic concern and the variable concern. In our previous section we talked about the generic concern which applies the same within this situation. Our second level of concern deals with the variable concern which in this case deals with an individual's income based on their age. Within our sample, we don't consider the entire broader population in the sample, we only consider the folks that are in the Boston area which means that the way that we relate the Boston individuals to the broader population may not hold. The train station environment, time of day, and initial attitude all play a strong factor in influencing the representativeness of the entire population. Another factor exists from having more young people chosen to respond by those handing out surveys like we discussed in our last model, what if were to assume that when the surveys are handed out randomly, younger people tended to fill out and submit them more than those who are older? Well, this would still skew the age distribution and overestimate younger people in the population, and if younger people also tend to have a lower income than older people, this could also alter our answers to our current questions.

If we had reason to believe this is true, one way that we could fix this issue of representativeness is to alter our population to be train commuters in the US who would respond to the survey. In doing so, our population would then accommodate for the skewed age distribution under the assumption that younger individuals tend to respond to surveys at higher rates than older people.

### Courage

```{r}
#| echo: false
#| fig.cap: Courage
knitr::include_graphics("other/images/Courage.jpg")
```

Once we have construct the mathematical structure of our model through the data generating mechanism we can use **Courage** to create a fitted model. The process of creating our fitted model will involve the transformation of several variables with a discussion and what variables to include and discard.

#### Mathematics

Let's interpret **courage** with with a look at the mathematics:

The mathematics for a continuous predictor is unchanged from the intercept-including example we explored previously.

$$income_i = \beta_0  + \beta_1 age_i + \epsilon_i$$

When comparing two people (persons 1 and 2), the first one year older than the second, $\beta_1$ is the expected difference in their incomes. The algebra is simple. Start with the two individuals.

$$income_1 = \beta_0  + \beta_1 age_1$$ $$income_2 = \beta_0  + \beta_1 age_2$$ We want the difference between them, so we subtract the second from the first, performing that subtraction on both sides of the equals sign.

$$income_1 - income_2 = \beta_0  + \beta_1 age_1 - \beta_0 - \beta_1 age_2\\
income_1 - income_2 = \beta_1 age_1 - \beta_1 age_2\\
income_1 - income_2 = \beta_1 (age_1 - age_2)$$

So, if person 1 is one year older than person 2, we have:

$$income_1 - income_2 = \beta_1 (age_1 - age_2)\\
income_1 - income_2 = \beta_1 (1)\\
income_1 - income_2 = \beta_1$$

The algebra demonstrates that $\beta_1$ is the same for all ages. The difference in expected income between two people aged 23 and 24 is the same as the difference between two people aged 80 and 81. Is that plausible? Maybe. *The algebra does not lie.* When we create a model like this, this is the assumption we are making.

Note how careful we are not to imply that increasing age by one year "causes" an increase in income. That is nonsense! *No causation without manipulation.* Since it is impossible to change someone's age, there is only one potential outcome. With only one potential outcome, a causal effect is not defined.

As important as our mathematics is, we need to take a look at the parameters at hand. Looking at our parameters allows us to create fitted model by combining the mathematics and interpretations of the parameters. $\beta_0$ is the difference in expected income between individuals. $\beta_1$ is the average age within all of the people in the dataset. As shown in our model, $\beta_0$ and $\beta_1$ are two terms that allow us to make our model. The second part in the right-hand side, $\epsilon$ ("epsilon"), represents the unexplained part of the outcome and is called the error term. In other words, $\sigma$ is what plays a role with age that is not factored into our model. We assume that this error follows a normal distribution with an expected value of 0 (meaning it is 0 on average) and it is simply the difference between the outcome and our model predictions. Therefore, we have three main parameters: $\beta_0$, $\beta_1$, and $\sigma$ that we will use within this section. 

#### Fitted Model

With **Justice** satisfied, we can use **Courage** to fit the model. The differences lie in how we interpret the results not with the creation of the model. The use of `stan_glm()` is the same as usual.

```{r}
fit_3 <- stan_glm(income ~ age, 
                  data = trains, 
                  seed = 28,
                  refresh = 0)

print(fit_3, details = FALSE)
```

*When comparing two individuals, one 30 years old and one 40, we expect the older to earn about \$9,000 more. But we are far from certain: the 95% confidence interval ranges from -\$3,000 to \$20,000.*

The above is a good summary of the models.

-   It is brief! No one wants to listen to too much of your prattle. One sentence gives a number of interest. The second sentence provides a confidence interval.

-   It rounds appropriately. No one wants to hear a bunch of decimals. Use sensible units.

-   It does not just blindly repeat numbers in the printed display. A one year difference in age, which is associated with a \$900 difference in income, is awkward. (We think.) A decade comparison is more sensible.

-   "When comparing" is a great phrase to start the summary of any non-causal model. Avoid language like "associated with" or "leads to" or "implies" or anything which even hints at a causal claim.

#### Model Checks

With the fitted model that we have created we are able to perform model checks. The model checks that we perform will help us understand how accurate our model is because we need to be able to ensure that our fitted model is reasonably accurate. We can view our model through model checks that interpret the residuals (error), outcome, and "fake-data" simulations. 

Consider our usual decomposition of the outcome into two parts: the model and the error term.

```{r}
#| echo: false
outcome <- trains |> 
  ggplot(aes(income)) +
    geom_histogram(bins = 50) +
    labs(x = "Income  in $1,000's",
         y = "Count") +
    scale_x_continuous(labels = scales::dollar_format(scale = 1/1000))

fitted <- tibble(fits = fitted(fit_3)) |> 
  ggplot(aes(fits)) +
    geom_histogram(bins = 50) +
    labs(x = "Fitted Values in $1,000's",
         y = NULL) +
    scale_x_continuous(labels = scales::dollar_format(scale = 1/1000))

res <- tibble(resids = residuals(fit_3)) |> 
  ggplot(aes(resids)) +
    geom_histogram(bins = 50) +
    labs(x = "Residuals in $1,000's",
         y = NULL) +
    scale_x_continuous(labels = scales::dollar_format(scale = 1/1000))
  
outcome + fitted + res +
  plot_annotation(title = "Decomposition of Income into Fitted Values and Residuals")
```

There are scores of different fitted values. Indeed, there are a greater number of different fitted values than there are different outcome values! This is often true for models which have continuous predictor variables, we have here with `age`.

Another model check that we will run is the posterior predictive check. With the help of the posterior predictive check we can run a "fake-data" simulation upon our fitted model that generates a distribution that allows us to view whether our fitted model is able to produce a distribution that is similar to the actual data distribution.

```{r}
#| echo: false
pp_check(fit_3, plotfun = "hist", nreps = 3, binwidth = 10000)
```

Our graph in the darker blue represents our actual data. As we can see with the lighter blue graph, our fitted model is able to generate a distribution for the values of income that very similar to the distribution of values for income for our actual data. When we take a look at the generated distribution that has been generated from our fitted model we can view that we don't have highly trusted responses to the results we have at hand. These three graphs that we have simulated all have data that is reasonably accurate when compared to the actual data, however, the residuals present within the fitted model will change the way that graph is presented.

#### Data Generating Mechanism (DGM)

Recall the Preceptor Table and the units from the beginning. We are concerned with three main ideas from the Preceptor Table: our units (dollars for the 40-year olds at the train station), our outcome (individual's income) covariates (the age of each individual). Now that we have run a posterior predict check to ensure that we have a fitted model that best suits us we can create a table like the one below to visualize the data we currently have. With the help of the data generating mechanism that we have created from our fitted model we can use this to fill in all of the missing values for our Preceptor Table. We use the data generating mechanism as the last step of **courage** as it allows us to analyze and view our fitted model's data. 

```{r}
#| echo: false
gtsummary::tbl_regression(fit_3, intercept = TRUE) %>%
  bold_labels()
```

`(Intercept)` corresponds to 103,062 which is $\beta_0$. As always, R has, behind the scenes, estimated the entire posterior probability distribution for $\beta_0$. We will graph that distribution in the next section. But the basic print method for these objects can't show the entire distribution, so it gives us summary numbers: the median and the MAD SD. Speaking roughly, we would expect about 95% of the values in the posterior to be within two MAD SD's of the median.

In summary: we model the income of individuals at a train station as a linear function of the age of each individual. We find that the average income for individuals at the train station approximates to 103,000 as well as, we can see that the older the age the more income seems to be.

<!-- Next step: A scatter plot with the fitted line drawn through it and a residual highlighted. Some more discussion! And some plots. Explain what the residuals are. And predicted values! Talk about sigma. More could be done. -->


### Temperance

```{r}
#| echo: false
#| fig.cap: Temperance
knitr::include_graphics("other/images/Temperance.jpg")
```

#### The Question


#### The Answer


#### Humility

**Courage** gave us the fitted model. With **Temperance** the posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect.

Recall the question:

*What would you expect the income to be for a random 40 year old?*

Given that we are looking for an expected value, we use `posterior_epred()`.

```{r}
newobs <- tibble(age = 40)

pe <- posterior_epred(fit_3, newdata = newobs) |> 
  as_tibble() 

pe
```

Plotting is the same as always.

```{r}
pe |> 
  ggplot(aes(x = `1`)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100)  +
    labs(title = "Posterior for Expected Income",
         subtitle = "A 40-years old commuter earns around $140,000",
         x = "Income",
         y = "Probability") +
    scale_x_continuous(labels = scales::dollar_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

<!-- More discussion! -->

Now that we have plotted all of our models and viewed our data we want to talk about the usage of these models and how **Temperance** plays a strong factor in the usage of these models. 

We should always be cautious about our inferences. Our assumptions are never true. We need to be cautious with our inferences when we deal with all of the assumptions that we have made within the chapter regarding validity, stability, representativeness, and model structure. When we take a look at the assumption of validity a possibility may not hold due could be the privacy of each station because the privacy of a train station differs from filling out surveys at home. When we take a look at the assumption of stability a possibility where it may not hold would be inflation and how that could impact income. When we take a look at the assumption of representativeness a possibility where it may not hold would be with correlation between our "sampling mechanism" and other variables to be true. When we take a look at the assumption of the model structure a possibility where it may not hold would be with the flaw of the fitted model where we conducted a posterior predict check that generated a distribution of values that showed the flaws within the model. Our stated inferences almost certainly underestimate the true uncertainty of the world. 

We need to maintain humility when we are making our inferences and decisions. Stay cautious my friends.

<!-- Insert meme! -->



## Summary

Throughout this chapter, we explored relationships between different variables in the `trains` data set. We built two predictive models and two causal models.

Similar to previous chapters, our first task is to always use **Wisdom**. We want to judge how relevant our data is based on the questions we ask. Is it reasonable to consider the data we have (e.g., income and age data from Boston commuters in 2012) are being drawn from the same population as the data we want to have (e.g., income and age data from today for the entire US)? Probably? Recall that these questions will bring up the **assumption of validity** and whether our data will be able to "stack" up with each other.

Our next step is to take a look at **Justice** which helps us decide the best way to represent the models that we will make. A little math won't kill you. Translating that math into code will be through the help of **Courage**. Our primary goal is to generate posterior distributions for the parameters and understand/interpret their meaning. Finally, we end off with **Temperance** that uses our models to answer the questions that we have asked above. Remember it is important to hone in on our questions to the most that we can because the most detailed questions will allow for the more accurate answers to prevail.

*Key Lessons and Commands That Were Talked About:*

-   Create a model using `stan_glm()`.

-   Use `posterior_epred()` to estimate expected values. The **e** in **e**pred stands for **e**xpected.

-   Use `posterior_predict()` to make forecasts for individuals. The variable in predictions is always greater than the variability in expectations because predictions can't pretend that $\epsilon_i$ is zero.

-   Once we have draws from a posterior distribution for our outcome variable --- whether that be an expectation or a prediction --- we can manipulate those draws to answer our question.

*Always Remember the Following:*

-   Always explore your data.

-   Predictive models care little about causality.

-   Predictive models and causal models use the same math and the same code.

-   "When comparing" is a great phrase to start the summary of any non-causal model.

-   Don't confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Be cautious in your use of the posterior.

```{r}
#| echo: false
#| cache: false
#| warning: false
knitr::write_bib(.packages(), "packages.bib")
```
