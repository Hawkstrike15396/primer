# Models {#sec-models}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(tidyverse)
library(gt)
library(gtsummary)
library(patchwork)
```

<!-- Basic plan is to first show the default brm. The model looks good for estimating $\rho$, and in fact matches what the PDF says. But when we try to use the model for prediction, it does poorly, because it things that biden is continuous rather than 0/1. pp_check shows this! So, we know to fix the model. Then we change the family to Bernoulli. Looks better! But how should we handle the link function. -->

<!-- Should we have more than one question at the start, or just show all the cool questions we can answer at the end? -->

<!-- Another paragraph or two of discussion about the final posterior. How do we use it? Within what range would you offer 50/50 odds that the true percentage lies? (use quantile(0.25, 0.75). Area under the curve? Explain clearly how that 26% is calculated. What is post_dist? What does it mean? How can I use it? Maybe show it sorted? Add some code comments. -->



In @sec-sampling, we learned about sampling, the process of gathering data to answer our questions. In this chapter, we will learn about constructing *models*, creating the data generating mechanism which we will use to answer our questions.

We always use the Cardinal Virtues. Wisdom helps us to clarify the questions with which we begin. We build the Preceptor Table which, if no data were missing, would allow us to answer the question. We check for validity. Justice creates the Population Table and examines the assumptions of stability, representativeness, and unconfoundedness. With Courage, we create a data generating mechanism. Temperance helps us to use that DGM to answer the questions with which we began.

As (almost) always, we start by loading the **tidyverse** package.

```{r}
#| code-fold: false
#| message: false

library(tidyverse)
```


## Cardinal Virtues

In @sec-sampling, we estimated the proportion, $\rho$, of red beans in an urn. In the real world, of course, we never mess with urns. But we are often interested in unknown proportions. For example, what proportion of US citizens support specific political candidates. Imagine that it is March 15, 2024 and we want to know $\rho$, the percentage of people who will vote for President Joe Biden in the 2024 presidential election. **How many beads are for Biden?**


Use the Cardinal Virtues to guide your thinking. 

### Wisdom

```{r}
#| echo: false
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of "validity," as to whether or not we can (reasonably!) assume that the two come from the same *population*.

#### Preceptor Table

A Preceptor Table is a table with rows and columns, such that, if no data is missing, we can easily answer our questions. 

```{r}
#| echo: false
tibble(voter_ID = c("1", "2", 
                   "...", "200", "201", "...", "2078", "2079", "..."),
       biden = c("0", "0", 
                   "...", "1", "0", "...", "1", "0", "...")) |>
  gt() |>
    tab_header(title = "Preceptor Table") |> 
    cols_label(voter_ID = "Voter ID",
               biden = "Voted for Biden") |>
    tab_style(cell_borders(sides = "right"),
              location = cells_body(columns = c(voter_ID))) |>
    cols_align(align = "center", columns = everything())
```

&nbsp;

The rows in the Preceptor Table correspond to . . . what, exactly? There are a lot of possibilities. They could include all those *eligible to vote* or all those actually *registered to vote* or all those who *actually did vote*. We need to be precise. Given the question, we only have rows for those who did vote in November 2024. In March, of course, we don't know who those rows will correspond to, nor even how many rows there will be.

The outcome in the Preceptor Table is the status, TRUE or FALSE, represented as `1` or `0`, of voting or not for Biden. Given the question, we don't need to know for whom each vote was cast. That is, the question does not, strictly speaking, even ask who will win the election. And that might be correct. But, as we go through the Cardinal Virtues, we need to constantly return to the starting question, often to modify it, given the limits of our data and our models. *You almost never start with the question that you eventually answer.*

There are no covariates or treatments to consider, so the Preceptor Table only has outcome (and ID) columns. There is only one outcome and so the model is predictive, not causal. A causal model requires (at least two) potential outcomes, each the result of a different treatment.

#### EDA

The data we have come from a [YouGov](https://today.yougov.com/) poll ([pdf](https://github.com/PPBDS/primer/blob/master/sources/you-gov-2024-03-12.pdf)) of 1,559 US adult citizens, conducted March 10 - 12, 2024.  There are many questions in the poll. For the purpose of answering our question, the most relevant one is:

> If an election for president were going to be held now and the Democratic nominee was  oe Biden and the Republican nominee was Donald Trump, would you vote for...

The allowed choice are "Joe Biden," "Donald Trump," "Other," "Not Sure," and "I would not vote." 42% of those polled indicated Joe Biden. Although rounding makes it impossible to know for sure, we will assume that 655 of the 1,559 "US adult citizens" would vote for Biden. Our data looks like:

```{r}
#| echo: false
tibble(poll_ID = c("1", "2", 
                   "...", "200", "201", "...", "1,559"),
       biden = c("0", "0", 
                   "...", "1", "0", "...", "1")) |>
  gt() |>
    tab_header(title = "Polling Data") |> 
    cols_label(poll_ID = "Poll ID",
               biden = "Would Vote for Biden") |>
    tab_style(cell_borders(sides = "right"),
              location = cells_body(columns = c(poll_ID))) |>
    cols_align(align = "center", columns = everything())
```

There are key differences between our data table and our Preceptor Table, despite their superficial similarities. 

* Our Polling Data includes exactly 1,559 rows. We don't know how many rows are in our Preceptor Table because we don't know, today, how many people will vote in November 2024.

* The ID column is labeled "Voter ID" in the Preceptor Table and "Poll ID" in the Polling Data. In many ways, the ID doesn't matter. The person labeled "2" in the first table, for example, has no necessary connection to the person labeled "2" in the second table. The numbers don't mean anything. But, conceptually, having different labels helps to highlight the issue of sampling and concerns about *representativeness*, concerns which we will address under Justice.

* The variable labels differ. "Voted for Biden" is not the same thing as "Would Vote for Biden." Indeed, the two columns represent very different things. 


#### Validity

The last step of Wisdom is to decide whether or not we can consider the units from the Preceptor Table and the units from the data to have been drawn from the same *population*. 
*Validity* involves the columns of our data set. Is the meaning of our columns consistent across the different data sources?  Or, rather, are they consistent enough to allow us to "stack" them on top of one another in order to construct the Population Table? Although there are some legitimate concerns, as discussed above, the validity assumption is close enough to true for us to proceed. 

### Justice

```{r}
#| echo: false
knitr::include_graphics("other/images/Justice.jpg")
```

Justice examines the assumptions of stability, representativeness, and unconfoundedness with regard to the Population Table.

#### Population Table

We use *The Population Table* to acknowledge the wider source from which we could have collected our data. It includes rows from three sources: the data for units we want to have (the Preceptor Table), the data for units which we have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or in the Preceptor Table). Consider:


```{r}
#| echo: false
tibble(source = c("...", "...", "...","...",  
                  "Data", "Data", "Data", "Data", "...",
                  "...", "...", "...","...", 
                  "Preceptor Table", "Preceptor Table", "Preceptor Table", "...",
                  "...", "...", "..."),
       time = c("February 2024", "February 2024", "February 2024", "...",
                "March 2024", "March 2024", "March 2024", "March 2024", "...",
                "October 2024", "October 2024", "October 2024", "...",
                "November 2024", "November 2024", "November 2024", "...",
                "December 2024", "December 2024", "December 2024"),
       id = c("1", "200", "976", "...",
              "1", "200", "...", "1559", "...",
              "1", "200", "2025", "...",
              "1", "200", "2078", "...",
              "1", "200", "2300"),
       biden = c("?", "?", "?", "...",
              "0", "1", "...", "1", "...",
              "?", "?", "?", "...",
              "1", "0", "1", "...",
              "?", "?", "?")) |>

  # Then, we use the gt function to make it pretty

  gt() |>
  cols_label(source = md("Source"),
             time = md("Time"),
             id = md("ID"),
             biden = md("Biden")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything())
```

&nbsp;

Every row in a Population Table corresponds to a unit/time combination. This is different from the Preceptor Table and the data, in both of which each row is for a unit. The entire Preceptor Table and data each have a specific time associated with them, even if that moment corresponds to a few days or longer. Nothing in this imperfect world is ever instantaneous. 

A Population Table will usually have several types of columns: id, time, covariates, and outcome(s). There are no covariates in this simple example, but we will use them in later chapters.

<!-- DK: A lot more to say here. -->

Now that we have created our Population Table, we can analyze the key assumptions of stability, representativeness and unconfoundedness.  

#### Stability

*Stability* involves time. Is the model --- meaning both the mathematical formula and the value of the parameters --- stable over time? 

#### Representativeness

*Representativeness* involves the data rows, specifically the rows for which we *have data* versus the rows for which we *might have had data*. Are the rows that we do have data for representative of the rows for which we do not have data? 


The *sampling mechanism* is the technical term for the process by which some people were sampled and some were not. We hope that all members of the population have the same chance of being sampled, or else our data might be unrepresentative of the larger population. Another term for this would be having a "biased" sample. Almost all samples have some bias, but we must make a judgement call to see if the data we have is close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population. 

#### Unconfoundedness

*Unconfoundedness* involves the possible correlation between treatment assignment and the potential outcomes. It is only a concern for causal models. Since this is a predictive model, we do not have to worry about unconfoundedness. There is no "treatment" which might be confounded with anything.



### Courage

```{r}
#| echo: false
knitr::include_graphics("other/images/Courage.jpg")
```

Justice verifies the Population Table. Courage creates a mathematical model which connects the outcome variable to the covariates, if any. Then, using code, we create a fitted model, including posterior probability distributions for all the unknown parameters.

Given that the polling problem is conceptually similar to the beads-in-an-urn problem, it is useful to perform the same back-of-the-envelope calculation we did in @sec-sampling. To do that, we first need to create the tibble:

<!-- DK: I tried to have this be a character variable named bead with values red and white. But I could not get brms to give me predictions which were red and white. So, I figure that, if we can only get 0/1 predictions, we should give it 0/1 input data. Still need to explore this. How does brms handle multinomial models? -->

```{r}
poll_data <- tibble(biden = c(rep(1, 655), 
                              rep(0, 904)))

slice_sample(poll_data, n = 10)
```

First, calculate the mean. This is our estimate of the center of the posterior distribution for $\rho$, the unknown proportion of the voters who support Biden.

```{r}
mean(poll_data$biden)
```

42% of the sample support Biden. Second, calculate the standard error of this estimate, by taking the standard deviation of the data and dividing it by the square root of the number of observations.

```{r}
sd(poll_data$biden) / sqrt(length(poll_data$biden))
```

The standard error is about 1.25%. This suggests that a 95% confidence interval for the true value of $\rho$ would be:

```{r}
c(mean(poll_data$biden) - 2 * sd(poll_data$biden) / sqrt(length(poll_data$biden)),
  mean(poll_data$biden) + 2 * sd(poll_data$biden) / sqrt(length(poll_data$biden)))
```

The margin of error, which is another term for two times the standard error, is about 2.5%. This is less than the value of 3.5% given in the pdf. The reason for the difference involves all the technical adjustments which YouGov needs to make to their estimate. We ignore those problems and so our value is too small. 

<!-- DK: Discuss how sloppy we are with language. -->

The *data generating mechanism*, or DGM, is a mathematical formula which mimics the process by which the data comes to us. The DGM for sampling scenarios with only two possible values is called *Bernoulli*, usually denoted as: 

$$ red_i  \sim Bernoulli(\rho) $$
Each bead $i$ which we sample can be either red or white. It is convenient to define the model in terms of whether or not the bead was red. If bead $i$ is red, the value drawn is `1`, which is the standard way of representing `TRUE`. In other words, $red_i = 1$ means that bead $i$ was red. Similarly, a white bead is indicated with `0`, meaning that it is `FALSE` that the bead was red. $\rho$ is the only parameter in a Bernoulli model. It is the probability that a `1`, instead of a `0`, is drawn. That is, $\rho$ is the probability of a red bead, which is the same thing as the proportion of red beads in the urn, assuming again that the sampling mechanism is not biased.

We typical estimate the unknown parameter $\rho$ from the Bernoulli model by using a logit regression:

$$
\rho = \frac{e^{\beta_0}}{1 + e^{\beta_0}}
$$

There are generally two parts for a statistical model: *family* and *link function*. The family is the probability distribution which generates the randomness in our data. The link function is the mathematical formula which *links* our data to the unknown parameters in the probability distribution. We will be reviewing these concepts over and over again in the rest of the *Primer*, so don't worry if things are a little unclear right now.

<!-- DK: Is the above correct? Could we explain it better? Maybe use multiple link functions in this chapter? -->

#### Models

Load the [**brms**](https://paul-buerkner.github.io/brms/) package:

```{r}
#| message: false
library(brms)
```

The **brms** package provides a user-friendly interface to work with the statistical language [Stan](https://mc-stan.org/), the leading tool for Bayesian model building.

##### Gaussian

The key function in the **brms** package is `brm()`. 

<!-- DK: How do you get rid of all the output? Should we specify a seed? -->

```{r}
fit_1 <- brm(formula = biden ~ 1,
             data = poll_data)
```

Notes:

* `brm()` produces a great deal of output, as you can see above. This is useful if there is some sort of problem. But, most of the time, we don't care. So we suppress the output in the rest of the *Primer*.

* We almost always assign the result of a `brm()` call to an object, as here. By convention, the name of that object begins with "fit" because it is a **fit**ted model object. This model was created quickly but larger models take longer. So, we assign the result to an object so we don't need to recreate it.

* The first argument to `brm()` is `formula`. This is provided using the R formula syntax in which the dependent variable, `red` is separated from the independent variables by a tilde: `~`. This is the standard approach in almost all R model estimating functions. In this case, the only independent variable is a constant, which is represented with a `1`.

* The second argument to `brm()` is `data`, which is a tibble containing the data used to estimate the model parameters. The variables specified in the `formula` must match the variables in the tibble which is passed in, which is .

<!-- DK: Given the simple model we use first, maybe we can make the math above simpler first. -->

<!-- * DK: This goes below. The third argument to `brm()` is `family`, which specifies the mathematical structure of the model we are estimating, often including a `link` function. In this case, the family is Bernoulli and the link is logistic, which is termed "logit."  -->

<!-- DK: Need more details on the above! Maybe not use a family the first time, or a model?  -->

Print out the model object.

```{r}
fit_1
```

There are many details. We will take a first pass here, leaving out elements which we will return to in later chapters. Highlights:

* The "Family" is "gaussian," which is the default value for the `family` argument in `brms()`. In the next example, we will provide the `family` argument explicitly. Gaussian is another term for the normal distribution. This is almost certainly a mistake, but "discovering" this mistake and then fixing it is a useful exercise.

* The "Links" are "identity" for both "mu" and "sigma." Recall the the gaussian/normal distribution is specified as $N(\mu, \sigma^2)$ in which $\mu$, or "mu," is the parameter for the mean or center of the distribution and $\sigma$, or "sigma," is the parameter for the standard deviation or spread of the distribution. 

<!-- DK: ? Identity links are ones in which the mathematical connection between the  -->

* The "Formula" is `biden ~ 1`, which is what we passed in to the `formula` argument when we called `brm()`.

* The "Data" is `poll_data`, which is what we passed in to the `data` argument when we called `brm()`. Note the reminder that there are 1,559 observations. Never hurts to check that the reported number of observations matches what you expect.

* Fitting Bayesian models is computationally complex. The information about "Draws," and the values for "Rhat," "Bulk_ESS," and "Tail_ESS" refer to those complexities. The bottom message gives a basic explanation. Don't worry about any of that now.

* The "Intercept" is the key part of the model. Because the family is (incorrectly!) Gaussian and the link function an identity, the actual model we are estimating looks like:

$$ biden_i =  \mu + \epsilon_i $$

with $\epsilon_i \sim N(0, \sigma^2)$. $y_i$ is the height of male $i$. $\mu$ is true proportion of Biden voters. $\epsilon_i$ is the "error term," the difference between the vote of person $i$ and the true proportion of Biden voters. The obvious problem with this is that $\mu$, also called the "Intercept," is about 0.42. The value of `biden` is either `0` or `1`. So, the value of $\epsilon$ is either -0.42 or + 0.58. No other value is possible. But that makes no sense if $\epsilon_i \sim N(0, \sigma^2)$! This will cause problems soon.

* Ignore that Gaussian/Bernoulli issue, we have an "Intercept" (or $\mu$) value of 0.42. Note the complexity involved in even this simple model. We have multiple ways of describing the same concept: the true proportion of Biden voters. First, we have $\rho$, a mathematical symbol associated with Bernoulli models and to which we will return. Second, we have `1`, meaning the symbol in the formula: `biden ~ 1`. This is an "intercept only" model, one with no covariates. Third, we have $\mu$, the standard symbol for the intercept in a intercept-only linear or Gaussian model. Fourth, we have the word "Intercept," as in the printout from the fitted model object. In other words, $\rho$, $1$, $\mu$, and Intercept all refer to the same thing!

* The "Est.Error" is, more or less, the same thing as the standard error of our 0.42 estimate for the intercept. The full value is 0.0123, which is very close to the value we calculated by hand above.

* The **l**ower and **u**pper bounds of the 95% **c**onfidence **i**nterval are indicated by "l-95% CI" and "u-95% CI." The number also match, subject to rounding, our calculations.  (We can't use the by-hand calculations with more complex models, otherwise we would have no need for **brms**!)

* The "sigma" variable in the "Family Specific Parameters" section refers to out estimate of $\sigma$, given the assumed truth of a gaussian error term. Since that assumption is clearly false, we won't waste time interpreting the results here.

Once we have a fitted model, we can use that model, along with the R packages [**tidybayes**](https://mjskay.github.io/tidybayes/) and [**bayesplots**](https://mc-stan.org/bayesplot/), to answer various questions.

```{r}
#| code-fold: false
#| message: false
library(tidybayes)
library(bayesplot)
```

<!-- DK: At some point, we should deal with the unique theme of which bayesplot uses. I want a consistent theme throughout the book. -->

We can generate a posterior distribution for $\rho$ with:

```{r}
#| code-fold: false

fit_1 |> 
  add_epred_draws(newdata = tibble(.rows = 1))
```

The first argument to the key function, `add_epred_draws()`, is `newdata`. We often want to examine the behavior of the model with new data sets, data which was not used in model estimation. The object passed as `newdata` is generally a tibble, with a row corresponding to each set of values for the independet variables. In this case, we have an intercept-only model, which means that there are no independent variables, other than the intercept, which does not need to be explicitly passed in. So, `newdata` is a tibble with no variables and one row.

```{r}
#| code-fold: false

tibble(.rows = 1)
```

The resulting tibble includes 4,000 rows, each of which is a draw from the posterior distribution of $\rho$. (Recall that $\rho$ and $\mu$ refer to the same thing.) You can ignore that columns named `.row`, `.chain`, and  `.iteration`. The `.draw` variable just keeps count of the draws. The key variable is `.epred`, which us a draw from the posterior distribution of $\rho$. The "e" in `.epred` stands for expected value. Grahically, we have:


```{r}
#| code-fold: false

fit_1 |> 
  add_epred_draws(newdata = tibble(.rows = 1)) |> 
  ggplot(aes(x = .epred)) + 
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) + 
    labs(title = expression(paste("Posterior Probability Distribution of ", rho)),
         subtitle = "Distribution is centered at 42%",
         x = expression(paste("Proportion, ", rho, ", of Biden Voters")),
         y = "Probability") + 
  
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

Even more useful than posteriors of the parameters in the model are predictions we can make with the model. The key function for exploring those predictions is `add_predicted_draws()`. This function behaves similarly to `add_epred_draws()`. In both cases, we need to provide the value for the `newdata` argument, generally a tibble with a column for all the variables used in the fitted model. Again, because this is an intercept-only model, there is no need for columns in the `newdata` tibble. No variables are used in `fit_1`.



```{r}
#| code-fold: false
fit_1 |> 
  add_predicted_draws(newdata = tibble(.rows = 1))
```

The columns which result are the same as with `add_epred_draws()` except that `.epred` is replaced with `.prediction`. (The reason that all the variable names have a leading `.` is that this convention makes it less likely, but not imossible, that they will conflict with any variable names in the `newdata` tibble.)

A glance at the result shows the problem: `.prediction` takes on a variety of values. But that is impossible! By definition, the only possible values are 0/1. We must have made a mistake in specifying the model. Another way to see the model is to perform a "posterior predictive check," a comparison of model predictions with your actual data. The **bayesplot** package includes the `pp_check()` function for that purpose.

```{r}
#| code-fold: false

pp_check(fit_1)
```

$y$ is the original data. $y_rep$ is a replication of the data, using the `fit_1` model. In other words, we "pretend" that `fit_1` is the true model. We then create 10 versions of the output data, as created by `add_predictive_draws()`, each the same size are our original data, which is 1,559 observations in this case. We then graph both $y$ and the 10 versions of $y_rep$.

As the graphic makes clear, we have a serious problem. The data, meaning the variable `biden`, which is refered to as $y$ in the plot, is always either 0 or 1, hence the two spikes in the plot. $y_rep$, on the other hand, is a continuous variable with a range of -0.8 to about 1.8. That is a contradiction! If our model were true, then the replicated values --- the posterior predictions --- would look a lot like our original data.

Sadly, most usage of `pp_check()` does not produce such a clear cut conclusion. We need to decide if $y$ and $y_rep$ are "close enough" that our model can be used as if it were true.


##### Bernoulli

We have discovered that our initial assumption about the correct mathematical formula for the data generating mechanism was mistaken. Instead of a gaussion (or Normal) probability model, we should use a Bernoulli model. We must modify our call to `brm()` as follows:


```{r}
#| code-fold: false
#| message: false
fit_2 <- brm(formula = biden ~ 1,
             data = poll_data,
             family = bernoulli())
```

This call to `brm()` is the same as the previous one, except that we have specified the value of the `family` argument as `bernoulli()`. (Note that we can give just the name of the family, which is itself a function, or include the parentheses as in `bernoulli()`. The results are the same.) Recall that the default value of `family` is `gaussian()` or, equivalently, `guassian`.  In a Bernoulli data generating mechanism, we have:

$$ biden_i  \sim Bernoulli(\rho) $$

Each voter $i$ which we sample either supports Biden or they do not. If person $i$ supports Biden, the value drawn is `1`, which is the standard way of representing `TRUE`. In other words, $biden_i = 1$ means that person $i$ supports Biden. Similarly, a person not supporting Biden is indicated with `0`, meaning that it is `FALSE` that the person $i$ supports Biden. $\rho$ is the only parameter in a Bernoulli model. It is the probability that a `1`, instead of a `0`, is drawn. That is, $\rho$ is the probability that a person supports Biden or, equivalently, the proportion of all voters who support Biden.

Consider the fitted object:

```{r}
#| code-fold: false

fit_2
```

The output is, of course, very similar to that for `fit_1`. The "Family" is "bernoulli" rather than "gaussian." The "Links" are "mu = logit" instead of "mu = identity." There is no link for "sigma" because a Bernoulli model has only one parameter. The "Formula," "Data," and "Draws" are the same as before. In order to interpret the Intercept value of -0.32, we need to revisit the concept of a link function.

Via my friend Claude, we have:

> In statistical modeling, a link function is a mathematical function that relates the linear predictor (the linear combination of the predictors and their coefficients) to the expected value of the response variable. The link function is a key component of generalized linear models (GLMs) and is used to account for the relationship between the predictors and the response variable when the response variable follows a non-normal distribution or has a restricted range.

> The purpose of the link function is to transform the expected value of the response variable to a scale that is unbounded and continuous, allowing it to be modeled as a linear combination of the predictors. The choice of the link function depends on the distribution of the response variable and the assumptions made about the relationship between the predictors and the response.

Don't worry if this is difficult to understand, we will revisit in future chapters.

The default link function for a Bernoulli model is logit, as in:

$$
\rho = \frac{e^{\beta_0}}{1 + e^{\beta_0}}
$$
By definition, the parameter $\rho$ is only allowed to take values between 0 and 1. We want to constrain the model so that only these values are even possible. Using the Gaussian model was a bad choice because it allows for values anywhere from positive to negative infinity. The magic of the link function, as you can see, is that it can only ever produce a number close to zero, if $\beta_0$ is very small, or close to one, if $\beta_0$ is very large. In other words, the model structure (which is a "logit" function) "bakes in" the restrictions on $\rho$ from the very start. We call this a "link" function because it *links* the unknown parameters we are estimating ($\beta_0$ in this case) to the paramter(s) (just one parameter, $\rho$, in this case, ) which are part of the data generating mechanism. Once we have $\rho$ (or, more precisely, the posterior distribution of $\rho$) we no longer care about $\beta_0$.

We can now interpret the value of the Intercept (which is the same thing as $\mu$ in this case, but not the same thing as $\rho$) as the value of $\beta_0$. In fact, the most common mathematical symbol for the intercept of a statistical model is $\beta_0$.

<!-- DK: Get each equation to appear on its own line. -->

$$
\begin{eqnarray}
\rho &=& \frac{e^{\beta_0}}{1 + e^{\beta_0}}\
\rho &=& \frac{e^{-0.32}}{1 + e^{-0.32}}\
\rho &\approx& 0.42
\end{eqnarray}
$$
<!-- DK: maybe replace the above with. But it doesn't work -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \rho &= \frac{e^{\beta_0}}{1 + e^{\beta_0}}\ -->
<!-- \rho &= \frac{e^{-0.7}}{1 + e^{-0.7}}\ -->
<!-- \rho &\approx 0.33 -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- Maybe we need amsmath package? -->

0.42 is the same number we calculated by hand and the same result when we used `family = gaussian()`. We can calculate the values for the upper and lower 95% confidence intervals by plugging -0.22 and -0.42 into the same formula.

We can create the entire posterior distribution for $\rho$ using similar code as before:

```{r}
#| code-fold: false

fit_2 |> 
  add_epred_draws(newdata = tibble(.rows = 1)) |> 
  ggplot(aes(x = .epred)) + 
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) + 
    labs(title = expression(paste("Posterior Probability Distribution of ", rho)),
         subtitle = "Distribution is centered at 42%",
         x = expression(paste("Proportion, ", rho, ", of Biden Voters")),
         y = "Probability") + 
    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

If both `fit_1` and `fit_2` produce the same posterior distribution for $\rho$, then who cares which one we use? The answer is that `fit_2` produces reasonable posterior predictions:


```{r}
#| code-fold: false
fit_2 |> 
  add_predicted_draws(newdata = tibble(.rows = 1))
```

All the values of `.prediction` are either zero or one, as we expect. Consider:

```{r}
#| code-fold: false
pp_check(fit_2, type = "bars")
```

Using `type = "bars"` is a good choice for `pp_check()` when `family` is `bernoulli()`. Note how the 10 replications of $y_rep$ are indistinguishable from the true output values $y$. A good model produces replications, sometimes termed "fake data," which is indistinguishable for the actual data.


We now have our data generating mechansim:

$$ biden_i  \sim Bernoulli(\rho = 0.42) $$

<!-- tidy() produces ugly stuff here, so maybe don't introduce it now. -->

#### Bayesian framework

<!-- DK: Too abstract? Too long? -->

We are Bayesian statisticians who make Bayesian models. This means that we make specific assumptions and consider data to be fixed and parameters to be variable. One of the most important distinctions is that in Bayesian data science, **we don't know the values of our parameters**. 

Some non-Bayesian frameworks are concerned with the probability distribution of our observed data, but do not care much about the probability distribution for $\rho$ and assume it to be fixed. If $\rho$ is fixed, the equation above becomes one simple binomial distribution. Think of this as a standard 2 dimensional plot. 

We Bayesians consider our observed data to be fixed. We don't consider alternate realities where our observed data is different due to sampling variation. Instead, we are concerned with the probability distribution of our parameter. In our urn scenario, $\rho$ is variable, so we have to create a separate binomial distribution for each possible value of $\rho$. Think of this as a 3 dimensional joint distribution, as we created in @sec-n-models.

It is essential to understand the joint distribution and the posterior, two concepts Bayesians use to solve problems. We will provide quick a quick review here, including statistical notation that may be helpful to some. 

**The joint distribution**, $p(y|\theta)$, models the outcome $y$ given one or more unknown parameter(s), $\theta$. The equation illustrates exact same concept we addressed while discussing the distinctions of Bayesian science: because our parameters are variable, we have to create separate distributions for each potential value. Combining all these distributions together creates a joint distribution that is 3 dimensional when plotted. 

**The posterior**, $p(\theta|y)$, is the probability distribution of our parameter(s) $\theta$, created using data $y$ that updates our beliefs. We have referenced the posterior many times before, and this definition does not change its meaning. 

In our urn scenario, obtaining the posterior involves first creating many binomial distributions for each possible population proportion. This is the joint distribution, and it is a 3 dimensional model. We then select the distribution that corresponds with our data: 17 red beads are sampled. We can represent the posterior with the following:

$$\text{Prob}(\text{models} | \text{data} = 17)$$

This is equivalent to taking a 2 dimensional slice of the 3 dimensional model. We are left with a probability distribution for our parameter, $\rho$. 

#### `stan_glm()`

Given a data set to use and a mathematical formula to work with, the next step is to write some code. We will use the **rstanarm** package, which provides a user friendly interface to work with the statistical language Stan. 

**rstanarm** and Stan are appealing because they are powerful. Functions such as `stan_glm()` can do everything we did by hand in @sec-probability in a few lines of code. Because we will use a professional statistical library, the objects we make will become more complex. In this Chapter, we provide the steps for answering our questions. @sec-two-parameters will provide a more detailed explanation of the objects we will make. *To be clear, you do not need to fully understand this section or how this code works. This is an introduction, not a formal lesson.*


```{r}
#| message: false
#| code-fold: false
library(rstanarm)

fit_1 <- stan_glm(formula = red ~ 1, 
                  data = tibble(red = c(rep(1, 17), 
                                        rep(0, 33))),
                  family = binomial,
                  refresh = 0,
                  seed = 10) 
```


Recall that we assumed a binomial model for the data generating mechanism.  In `stan_glm()` we denote this with `family = binomial`. In addition to the type of the distribution, we also need to analyze the outcome and predictor variables involved. The outcome is the quantity we are measuring, in this case the total number of red beads in our sample.  Because we have no predictors, we use the argument `formula = red ~ 1`, which means that we only model the outcome based on the unknown proportion of red beads in the urn, $\rho$. 

We pass in data in a binomial format: the 1's represent the number of successes (red beads drawn), and the 0's represent the number of failures (white beads drawn). As such, we pass a tibble with 17 red beads and 33 white beads into `data`. 

We use `refresh = 0` to suppress the behavior of printing to the console, and `seed = 10` so that we get the same output every time we run the code. The resulting model is:

```{r}
fit_1
```

We will learn the meaning of this output in @sec-two-parameters. Once we have the `fit_1` object, it is easy to answer two sorts of questions: the posterior probability distribution for $\rho$ and predictions for new draws from the urn. The key functions are `posterior_epred()` for the former and `posterior_predict()` for the latter.
 
Let's create our posterior for $\rho$ by using `posterior_epred()`:

```{r}
#| code-fold: false
ppd_for_p <- posterior_epred(fit_1, 
                newdata = tibble(.rows = 1)) |> 
  as_tibble() |>
  rename(p = `1`)
  
# posterior_epred() will unhelpfully name the column of our tibble to "1". We
# have two options: either refer to the column name as `1`, or rename the column
# to make it less confusing. We will rename the column to "p" in this chapter, but you
# will oftentimes see `1` in later chapters.

ppd_for_p
```

Plot the result:   

```{r}
ppd_for_p |> 
  ggplot(aes(x = p)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) + 
    labs(title = "Posterior Probability Distribution",
         subtitle = "Distribution is centered at .34",
         x = "Proportion p of Red Beads in Urn",
         y = "Probability") + 
  
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

We have successfully created the posterior distribution and can finally answer the question we started the chapter with: 

> If we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion $\rho$ of the beads in the urn are red?

Look to the posterior probability distribution we have created. We can see that the bulk of the area under the posterior occurs approximately when $\rho$ is between .28 and .42, so the answer to our question is that it is likely that 28% to 42% of the beads in the urn are red. Is that specific range the truth? No! We are just eye-balling the distribution, giving a rough sense of the likely value of $\rho$. Any range that is large enough to acknowledge the uncertainty we have regarding the exact value of $\rho$ is acceptable. 

Although the most likely probability (the highest bar on the histogram) occurs when $\rho$ is around .34, The answer is not a single number. Our posterior distribution is just that: a distribution. Using our data, we have many different results for the proportion of red beads in the entire urn. Certain proportions, like the extremes close to 0% or 100%, are essentially impossible due to our sample value being 34%. On the other hand, we could have just as easily sampled 16 or 18 beads from the urn, and sample proportions such as 32% and 36% are very plausible.

This means that, while we can provide a range of possibilities (and we can estimate which of those possibilities occur most frequently), we can never say that we know the total number of red beads with certainty.  We know that there is the most chance that $\rho$ is between .28 and about .42, some chance that $\rho$ is between .15 and .24 or between .42 and .56, and almost no chance that $\rho$ is below .15 or above .56. With the posterior we can visualize all of these probabilities at once. 

Another important question remains: 

> Why are there 4,000 rows in the stan_glm() tibble?

By default, `stan_glm()` will sample from the posterior in 2 sets of 2,000 iterations. If needed we can change the default number of iterations using the `iter` argument, but there are few reasons to do so. Some of us may still want to know why we sample from the posterior in the first place. Why not use the entire posterior? The answer is that the posterior is a theoretical beast, which makes it difficult to work with. 

For example, what if we wanted to know the probability that $\rho$ is between .3 and .4? To answer this using the pure posterior, we would need to calculate the area under the distribution from when $.3 < \rho < .4$. This is more difficult then it seems, as the posterior is a distribution, so it has no individual observations to work with as it's continuous! 

Instead, we can work with draws from the posterior. With enough draws we create a close approximation of the posterior which models the counts of our observations. This is an *approximation*; it is not exactly the posterior, but close enough for our purposes. We can easily convert our posterior distribution into a **posterior probability distribution**, by making the area under the graph sum to 1. The posterior probability distribution is often used as a visual aid, as percentages are more easy to conceptualize than raw numbers. One way to convert a posterior distribution into a probability distribution is to group by each value of $\rho$ and turn the counts into probabilities: 

```{r}
#| code-fold: false
ppd_for_p |>
  round(digits = 2) |>
  summarize(prob = n()/nrow(ppd_for_p),
            .by = p) |>
  arrange(desc(prob))
```

We can also accomplish a similar effect by passing `aes(y = after_stat(count/sum(count))` into `geom_histogram()` when plotting.  Oftentimes, like in answering the probability that $\rho$ is between .3 and .4,  we can work with the posterior distribution to the very end. Just divide the number of draws that meet our condition (are between .3 and .4), by the total number of draws.   

```{r}
sum(ppd_for_p$p > .3 & ppd_for_p$p < .4)/nrow(ppd_for_p)
```

There is approximately a 54% chance that $\rho$ is between .3 and .4. Give me enough draws from the posterior and I can show you the world!


### Temperance

```{r}
#| echo: false
knitr::include_graphics("other/images/Temperance.jpg")
```

With the fitted model object `fit_1`, we can answer our questions.

Recall the second question we started with:

> What is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?

#### Using the posterior

Whenever someone asks you a question, you need to decide what posterior probability distribution would make it easy for you to answer that question. In this case, if we know the posterior probability distribution for the number of red beads in a shovel of size 20, then a question about the likelihood of drawing more than 8 (or any other value) is easy to answer.

The posterior probability distribution for a probability is a tricky thing. It is much easier just to estimate the posterior probability distribution for the outcome --- number of red beads out of 20 --- and then work with that distribution in order to answer probability-type questions.

To predict these future unknown samples, we use `posterior_predict()`. We pass the posterior created using `stan_glm()` as the first argument, and because we want to estimate the number of red draws with a shovel size of 20, we use pass a tibble with 20 rows into `newdata`.  

```{r}
#| code-fold: false
posterior_predict(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble()
```

Each of our 4,000 rows represent one instance of us predicting a future sample from the urn, and each column represents the color bead in a shovel slot. We will create a new column called `total`, using `rowwise()` with `c_across()` to calculate the total number of red beads drawn in the sample. Finally, we will graph the resulting distribution.  

```{r}
ppd_reds_in_20 <- posterior_predict(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |> 
  select(total)


ppd_reds_in_20   |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Number of red beads in 20-slot shovel",
         x = "Number of Red Beads",
         y = "Probability") +  
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```

We have successfully created the posterior probability distribution for the number of red beads drawn in a shovel of size 20. But before we answer our question, some of us may be wondering why we made our predictions using `posterior_predict()` instead of `posterior_epred()`. Let's examine what happens if we use `posterior_epred()` instead.

```{r}
#| fig.cap: Using posterior_epred()

post_epred <- posterior_epred(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |> 
  select(total)

post_epred  |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
    labs(title = "Posterior probability distribution using posterior_epred()",
         subtitle = "In our scenario, using posterior_epred() is incorrect",
         x = "Number of red beads",
         y = "Probability") + 
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```


What happened? `posterior_epred()` shows the distribution of the **entire population**, which is continuous. The expected predictions can be fractional, because `posterior_epred()` returns draws from the posterior (which can be fractional) contingent on some covariate. In our scenario we have no covariates from which to create expected predictions, so `posterior_epred()` just returns the posterior, but re-scaled to between 0 and 20 beads instead of between 0 and 1 as before. The shape of the distributions are identical: 

```{r}
post_epred |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) + 
  labs(x = "Number of red beads sampled out of 20",
       y = "Probability") + 
  ppd_for_p  |>
  ggplot(aes(x = p)) + 
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
  labs(x = "Proportion red in urn",
       y = "Probability") + 
  plot_annotation(title = expression(paste("Expected prediction for sample of size 20 on left, posterior distribution for ", rho, " on right.")),
                  subtitle = "The two distributions have an identical shape.")
```

On the other hand, `posterior_predict()` models the posterior distribution for future **individuals**. In our scenario, we model the binomial distribution of a discrete random variable. The bars only appear at real numbers between 1 and 16, because we are predicting the probability of individual samples. We cannot draw fractions of beads in our sample. Using `posterior_predict()` essentially replicates the DGM, taking many virtual draws from our urn and summarizing all the results. 

In summary, use `posterior_predict()` when to predict the outcome of individual(s) in the future, and use `posterior_epred()` to model the probability across the entire population using the posterior. To answer our question, we want to know the probability of outcomes using a single shovel of size 20. We should use `posterior_predict()` to model taking individual samples many times, and we can then analyze the probabilities. If this is confusing do not fret! We will have plenty of practice with these 2 functions for the remainder of this *Primer*. 

Now let's attempt to actually answer our question: 

> What is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?

Because `posterior_predict()` takes predictive draws for us, we can simply count the number of draws that have more than 8 red beads, and divide by the total number of draws. 

```{r}
# Same code as earlier, included as a refresher. 

ppd_reds_in_20 <- posterior_predict(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |> 
  select(total)

# Calculating the probability

sum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total)
```

**There is approximately a `r round(100 *sum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total))`% chance that we will draw more than 8 red beads out of a sample size of 20.**

To visualize this probability graphically, we will reuse our posterior, and add a new column called `above_eight` that is `TRUE` if `total > 8`. 

```{r}
ppd_reds_in_20 <- posterior_predict(fit_1, 
                    newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |>
  select(total) |>
  mutate(above_eight = ifelse(total > 8, TRUE, FALSE))

ppd_reds_in_20
```

We can then can set the fill of our histogram to when `above_eight == TRUE` to visualize the probability of drawing more than 8 red beads. 

```{r}
ppd_reds_in_20   |> 
  
  # Set fill as above_eight. 
  
  ggplot(aes(x = total, fill = above_eight)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
  
  # Scale_fill_manual()  is calling grey for the first color and red for the
  # second color. This is going to highlight the portion of the curve that we
  # want to highlight in red.

  scale_fill_manual(values = c('grey50', 'red')) +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Number of red beads in 20-slot shovel",
         x = "Number of Red Beads",
         y = "Probability",
         fill = "More than 8 Red Beads Drawn?") +  
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```

The red bars illustrate the area under a specific section of the curve, as compared to the *entire* area under the curve. *Each question requires looking at a new area under the curve.* When someones asks you a question, they are doing two things. First, they are providing instructions as to the posterior your should create. Here, the results with a shovel of 20 slots. Second, they are asking a question about the area under the curve in a specific region. Here, the region where the number of red beads is greater than 8 is highlighted in red. Therefore, the area below the curve that **is red** is how we get our estimate. 





## Summary

In this chapter, we performed both tactile and virtual sampling exercises to make inferences about an unknown parameter: the proportion of red beads. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion $\hat{\rho}$ to estimate the true proportion $\rho$. However, we are not just limited to scenarios related to proportions. We can use sampling to estimate other unknown quantities.

There is a truth! There is a true value for $\rho$ which we do not know. We want to create a posterior probability distribution which summarizes our knowledge. We care about the posterior probability distribution of $\rho$. The center of that distribution is around the mean or median of the proportion in your sample. The sd (or mad) of that posterior is the standard deviation divided by the square root of our sample size. Note that this is the same thing as the standard deviation of the repeated samples.

We journey from reality, to our predictions, to the standard error of our predictions, to the posterior probability distribution for $\rho$. This is our sequence:

**$\rho$ (i.e., the truth) $\Rightarrow$ $\hat{\rho}$ (i.e., my estimate) $\Rightarrow$ the standard error of $\hat{\rho}$ (i.e., black box of math mumbo jumbo and computer simulation magic) $\Rightarrow$ our posterior probability distribution for $\rho$ (i.e., our beliefs about the truth).**

This journey shows how our beliefs about the truth develop through our work. We begin with $\rho$; $\rho$ is the truth, the true but unknown value we are estimating. $\hat{\rho}$ is our estimate for $\rho$. There can be millions and millions of $\hat{\rho}$'s. Next, we must estimate the standard error of our estimates (our $\hat{\rho}$'s) to account for the uncertainty of our predictions. Finally, we create a posterior probability distribution for $\rho$. This distribution is used to answer any questions about $\rho$. 


### Other highlights

- Standard error is just a fancy term for your uncertainty about something you don't know. Standard error $\approx$ our (uncertain) beliefs.

- Larger sample sizes $\implies$ lower standard errors $\implies$ more accurate estimates.   

- If we could only know two pieces of information from our data, we would need a measure of the **center** of the distribution (like mean or median) and a measure of the **variability** of the distribution (like sd or MAD). 

- The standard error refers to the standard deviation of a sample statistic (also known as a "point estimate"), such as the mean or median. Therefore, the "standard error of the mean" refers to the standard deviation of the distribution of sample means taken from a population. 

- `stan_glm()` can create a joint distribution and then estimate the posterior probability distribution, conditional on the data which was passed in to the data argument. This is a much easier way to create the posterior distribution, and will be explored in more detail in @sec-two-parameters. 

- We use the posterior distribution to answer our questions. 

As we continue our journey, recall the case of Primrose Everdeen and what she represents: no matter how realistic our model is, our predictions are **never certain**.  


