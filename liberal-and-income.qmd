# Liberal and Income 

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(gt)
library(gtsummary)
library(patchwork)
```

<!-- Is the chapter too long? Should I just have a single question for each section? Should I cut out one of the sections entirely? -->

<!-- Standardize years. Data is from 2012. Questions are about 2024. -->

<!-- Remove sampling & assignment mechanism -->

<!-- Come up with common ways in which a lack of representativeness causes problems. This is a list of things that we can check each time when we are interrogating a data science problem. Key issue is the correlation, if anything, between the "assignment mechanism" or "sampling mechanism" and other variables. Discuss sampling mechanism in all cases except att_end ~ treatment. For that, use "assignment mechanism." -->

<!-- Make residual plots consistent in this chapter! Also, we should ensure that the width of 10 units is the same in each plot. Or does that already happen? Want the intuition that, the more predictive the model, the smaller the remaining variation in the residuals.  -->

<!-- Whenever we have a predictive model, use language like "when comparing." Avoid even the hint of a causal claim. -->

Models have parameters. In @sec-sampling we created a model with a single parameter $\rho$, the proportion of red beads in an urn. In @sec-two-parameters, we used models with two parameters: $\mu$ (the average height in the population, generically known as a model "intercept") and $\sigma$ (the variation in height in the population). Here --- can you guess where this is going? --- we will build models with three parameters: $\sigma$ (which serves the same role throughout the book) and two "coefficients." In models which relate a predictor to the outcome, those two parameters will be labeled $\beta_0$ and $\beta_1$. All this notation is confusing, not least because different academic fields use inconsistent schemes. Follow the Cardinal Virtues and tackle your problem step by step.

Perhaps more importantly, a focus on parameters is less relevant now than it was decades ago, when computational limitations made answering our actual questions harder. *Parameters are imaginary.* They don't exist. They, in general, are not the answer to a real world question. They are tools, along with the models of which they are a part, we use to answer questions.

Packages:

```{r}
#| message: false
#| code-fold: false
library(primer.data)
library(tidyverse)
library(brms)
library(tidybayes)
```

We are concerned with these three packages because we are exploring the `trains` dataset which exists within the **primer.data** package. We use the **brms** package to build Bayesian models. ("brms" stands for **B**ayesian **r**egression **m**odel**s**.) The **tidybayes** packages makes working the fitted models easierr. As usual, we use the **tidyverse** package.

In this chapter, we are going to ask a series of questions involving train commuters' ages, party affiliations, incomes, and political ideology, as well as the causal effect of exposure to Spanish-speakers on their attitude toward immigration. These questions will pertain to all train commuters in the US today. For a refresher on this data, refer to @sec-rubin-causal-model.

## liberal \~ income

So far in this chapter, we have only considered *continuous* outcome variables. `age`, `att_end` and `income` all take on a variety of values. None of them are, truly, continuous, of course. `age` is only reported as an integer value. `att_end` can only, by definition, take on 13 distinct values. However, from a modeling perspective, what matters is that they have more than 2 possible values.

`liberal`, however, only takes on two values: TRUE and FALSE. In order to model it, we must use the `binomial` family. We begin, as always, with some questions:

*Among all people who have an income of \$100,000, what proportion are liberal?*

*Assume we have a group of eight people, two of whom make \$100,000, two \$200,000, two \$300,000 and two \$400,000. How many will be liberal?*

We can answer these and similar questions by creating a model that uses party affiliation to predict age. Let's follow the four Cardinal Virtues: Wisdom, Justice, Courage and Temperance.

### Wisdom

```{r}
#| echo: false
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of "validity," as to whether or not we can (reasonably!) assume that the two come from the same *population*.

#### Preceptor Table


First, do we need a **causal** or **predictive** model? In our question above we have a predictive model. When we are deciding between a causal or predictive model we want to look at the question. Does it include key words such as: "cause," "affect," or "influence." Often these key words indicate that we are dealing with a causal model were a comparison exists between two states of a variable.

Second, what is the outcome? A person's attitude would be the **outcome** in this scenario. When we take a look at the questions we discover that there is a flaw that exists. It is vital to understand where and when that we are answering these questions because otherwise it could apply to the whole country, nation, or specific state. Therefore, by diving deeper within these questions we are able to accurately answer our question with the population that it refers to. For the questions that we have at hand we will be talking about all the adults in July 1, 2012 at the location of the United States which will allow us to answer the questions more accurately. 

Third, what are the **units**? Our units for this scenario would be individuals because the questions are about the attributes of unique people at the station.

Fourth, do we have a **treatment**? No. When we deal with predictive models we don't have any treatments. In this case, the treatment does not apply as we aren't dealing with a covariate that we *can* manipulate and *need* to manipulate. 

Let's look at our refined question to create our Preceptor Table:

*Among all people in the United States in 2012 who have an income of \$100,000, what proportion are liberal?*

Our Preceptor Table:

```{r}
#| echo: false
tibble(ID = c("1", "2", "...", "10", "11", "...", "N"),
       liberal = c("0", "0", "...", "1", "1", "...", "1"),
       income = c("150000", "50000", "...", "65000", "35000", "...", "78000")) |>
  gt() |>
  tab_header(title = "Preceptor Table") |> 
  cols_label(ID = md("ID"),
             liberal = md("Liberal"),
             income = md("Income")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(liberal)) |>
  tab_spanner(label = "Covariate", columns = c(income))
```

Note: the values that have a star next to them symbolize the possible values that may exist if they were "control" instead of "treated" or vice versa.

Recall: a Preceptor Table is the smallest table with rows and columns such that, if there is no missing data, all our questions are easy to answer. To answer questions — like “Among all people who have an income $100,000, what proportion are liberal?
and Assume we have a group of eight people, two of whom make $100,000, two $200,000, two $300,000 and two $400,000. How many will be liberal?” — we need a row for every individual at the station.

#### Data Analysis with Exploratory Approach

Recall the discussion from @sec-rubin-causal-model. @enos2014 randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. The data that we want to analyze consists of the age and party of each individual on these train platforms based on our variables in our Preceptor Table. These reactions were measured through changes in answers to three survey questions.

```{r}
trains |>
  select(liberal, income)
```

As specified in the Preceptor Table we only care about `liberal` is whether they are liberal or not and `income` is the income of the respondent. `liberal` is shown to be `TRUE` when the individual is liberal and `FALSE` when an individual is not liberal, playing a role in shaping their attitude towards the immigrants.

```{r}
trains |>
  select(liberal, income) |>
  summary()
```

The range for `income` seems reasonable.


```{r}
trains |>
  ggplot(aes(x = income, y = liberal)) + 
  geom_point() + 
  labs(title = "Income Based On Liberal",
       x = "Income in 1000s",
       y = NULL,
       fill = NULL) + 
  scale_x_continuous(labels = scales::dollar_format(scale = 1/1000)) +
  scale_y_discrete(labels=c("TRUE" = "Liberal", "FALSE" = "Not Liberal"))
```


Our plot above shows a wide assortment of incomes for both liberals and non-liberals, however, there is no clear connection between liberal and income. As we can see on the higher end there are both individuals that are liberal and not liberal that are attaining this sort of income. On the lower end it is the same, as a result we can see that a strong connection between liberal and income clearly does not exist within our situation. 

#### Validity

Now let's take a look at the **assumption of validity** for our data and Preceptor Table. There is no *truth* here. The **assumption of validity** may not hold due to the possibility of how our data was collected. We asked each individual whether they were "liberal," which means that we didn't rely on the government database. The possibility of the data being different from the government database and the way that we asked each individual may differ across both the Preceptor Table and the data which can prove the assumption of validity to be false.

Another case where the **assumption of validity** may not hold would be from how our income data was collected similar to our age data collection. The data has been collected from filling out surveys about income without cross-checking with the government database. We don't know whether our income is accurate enough when compared to the government's database to the survey that each individual filled out.

Despite these concerns, we will assume that validity does hold between the relationship of `liberal` and `income` such so we can "stack" our data and the Preceptor Table on top of each other.

### Justice

```{r}
#| echo: false
#| fig.cap: Justice
knitr::include_graphics("other/images/Justice.jpg")
```

Let's now consider Justice for the relationship between `liberal` and `income`. In **Justice**, we must consider the Population Table, stability, representativeness, unconfoundedness and the mathematical structure of the data generating mechanism (DGM).

#### Population Table

With the **assumption of validity** seeming reasonable, we can now create our Population Table. The Population Table consists of all the rows from the Preceptor Table, all the rows from our data, and all the rows from the broader population from which both are drawn.

```{r}
#| echo: false
tibble(source = c("...", "Data", "Data", "...", 
                  "...", "Preceptor Table", "Preceptor Table", "..."),
       att_yes_liberal = c("...", "3", "4", "...",
                           "...", "...", "...", "..."),
       att_no_liberal = c("...", "6*", "9*", "...",
                          "...", "...", "...", "..."),
       city = c("...", "Boston, MA", "Boston, MA", "...", 
                "...", "Chicago, IL", "Chicago, IL", "..."),
       year = c("...", "2012", "2012", "...", 
                "...", "2024", "2024", "..."),
       income = c("...", "150000", "50000", "...",
                 "...", "...", "...", "...")) |>
  gt() |>
  tab_header(title = "Population Table") |> 
  cols_label(source = md("Source"),
             att_yes_liberal = md("Liberal Ending Attitude"),
             att_no_liberal = md("Not Liberal Ending Attitude"),
             city = md("City"),
             year = md("Year"),
             income = md("Income")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Potential Outcomes", columns = c(att_yes_liberal, att_no_liberal)) |>
  tab_spanner(label = "Covariates", columns = c(income, year, city))
```

Note: the values that have a star next to them symbolize the possible values that may exist if they were "control" instead of "treated" or vice versa.

Our year within the Population table is an example of the moment in time.

Look at the **validity** of the `liberal` variable. This column has the values "Liberal" and "Not Liberal" to convey the political ideology of the train commuters. We must determine if the column `liberal`, and therefore the meaning of being liberal, is the same in Boston in 2012 as in the US in 2024. If we can determine these to be the same, then we can assume validity, and in this case, because the core beliefs of being liberal have not changed very much between 2012 and 2024, we can determine that the data is valid.

#### Stability

Let's look at **stability** with the relationship between `liberal` and `income` and determine whether or not we believe that this relationship has changed between 2012 and 2024. With our knowledge of the world, do we have any reason to believe that this has changed? What if between 2012 and 2024, income increased for those who are liberal? Well, then our model for the the relationship between `income` and `liberal` would have changed over the years, and therefore so would the model. However, since with our knowledge of the world we have no reason to believe that something of the sorts happened to affect this relationship, we can consider our model to be stable.

#### Representativeness

**Representativeness** has to do with how well our sample represents the larger population we are interested in generalizing to. When we deal with representativeness we deal with two potential problems: Is our data representative of the population? Is our Preceptor Table representative of the population? In our past three situations, we have considered two possible issues that we could have with the **representativeness** of our model, such as the difference between Boston and other cities and bias in those who respond. We will now consider how in this survey there were surveys given before and after treatment and some people may have filled out only one of the surveys. People who only filled out one of the surveys could affect the **representativeness** of the data because they could not be included in the data and if those who only filled out one survey tended to be liberal, then this would affect our data because it would underestimate the amount of liberals in the survey. This is something we must consider when looking at **representativeness**, since we could otherwise not determine if this data from train commuters in Boston in 2012 is representative enough of train commuters in the US now to continue using our data. Our generic level of concern has been seen from the previous sections. When we take a look at our variable concern and whether an individual is liberal and an individual's income, the way than an individual may define liberal can change across Chicago making the sample that we have not representative of the broader population. 

### Courage

```{r}
#| echo: false
#| fig.cap: Courage
knitr::include_graphics("other/images/Courage.jpg")
```

We will use the mathematical structure that through the data generating mechanism to generate a fitted model with the help of **Courage**. Courage allows us to explore different models. We can avoid hypothesis tests as we check our models for consistency with the data we have.

#### Mathematics

Let's consider whether this model is linear or logistic. Unlike our previous models this chapter, the outcome variable, `liberal`, for this model, only has two options, "Liberal" or "Not Liberal". Therefore, this will be logistic because there are only 2 possible outcomes and the outcome variable isn't continuous.

Recall the discussion in @sec-zero-one-outcomes about the logistic regression model which we use whenever the outcome or dependent variable is binary/logical. The math is there, if you care about math. We don't, at least not too much. Reminder:

$$p(\text{Liberal}_i = \text{TRUE}) = \frac{\text{exp}(\beta_0 + \beta_1 \text{income}_i)}{1 + \text{exp}(\beta_0 + \beta_1 \text{income}_i)}$$

This model only has two parameters, $\beta_0$ and $\beta_1$. But these parameters do not have simple interpretations, unlike the parameters in a linear (or gaussian) model.

Recall the fundamental structure of all data science problems:

$$\text{outcome} = \text{model} + \text{what is not in the model}$$ The exact mathematics of the model --- the parameters, their interpretations --- are all just [dross](https://en.wikipedia.org/wiki/Dross) in the foundry of our inferences: unavoidable but not worth too much of our time.

Even if the math is ignorable, the **causal** versus **predictive** nature of the model is not. Is this a causal model or a predictive model? It depends! It could be causal if you assume that we can *manipulate* someone's income, if, that is, there are at least two potential outcomes: person $i$'s liberal status if she makes X dollars and person $i$'s liberal status if she makes Y dollars. Remember: *No causation without manipulation*. The definition of a causal effect is the difference between two potential outcomes. If you only have one outcome, then your model can not be causal.

In many circumstances, we don't really care if a model is causal or not. We might only want to forecast/predict/explain the outcome variable. In that case, whether we can interpret the influence of a variable as causal is irrelevant to our use of that variable.

#### Fitted Model

Fitting a logistic model is easy. We use all the same arguments as usual, but with `family = binomial` added.

```{r}
fit_4 <- stan_glm(data = ch5,
                  formula = liberal ~ income,
                  family = binomial,
                  refresh = 0,
                  seed = 365)
```

Having fit the model, we can look at a printed summary. Note the use of the `digits` argument to display more digits in the printout.

```{r}
print(fit_4, digits = 6)
```

Fitted models tell us about the posterior distributions for the parameters in the formula which defines the model we have estimated. *We are assuming that the model is true.* Yet that assumption is always false! Our model is never a perfectly accurate representation of reality. But, if it were perfect, then the posterior distributions which we have created for $\beta_0$, $\beta_1$, and so on would be perfect as well.

When working with a linear model, we will often interpret the meaning of the parameters, as we have already done in the first three sections of this chapter. Such interpretations are much harder with logistic models because the math is much less convenient. So, we won't even bother to try to understand the meaning of these parameters. However, we can note that $\beta_1$ is negative, suggesting that people with higher incomes are less likely to be liberal.

#### Model Checks

With the fitted model that we have created we are able to perform model checks. Model checks help us understand how accurate our model is to ensure that the fitted model that we have created is reasonably accurate when compared to the actual data. We can view our model through the posterior predictive check that simulates the data upon our fitted model to generate a distribution. With the posterior predictive check we are able to visualize how accurate our data is compared to the actual data ensuring that we have created a great fitted model.

```{r}
#| echo: false
pp_check(fit_4, plotfun = "hist", nreps = 3, binwidth = 0.1)
```

Our graph in the darker blue represents our actual data. As we can see with the lighter blue graph, our fitted model is able to generate a distribution for liberal that is similar to the actual data distribution as shown above. For the most part the model is really accurate as we are able to see the predictions that seem to be highly accurate. 

#### Data Generating Mechanism (DGM)

Recall the Preceptor Table and the units from the beginning. We are concerned with three main ideas from the Preceptor Table: our units (individuals with unique attributes at the train station), our outcome (individual's attitude toward immigration) covariates (the treatment that each individual will receive). Now that we have run the necessary model checks on our fitted model to ensure that we have the best suited model, we can create a table to view our data. With the accurate fitted model and low residuals proving to be accurate we can use the data generating mechanism to finish the last step in the **Courage** process.

```{r}
#| echo: false
gtsummary::tbl_regression(fit_4, intercept = TRUE) %>%
  bold_labels()
```

`(Intercept)` corresponds to 0.56 which is $\beta_0$. As always, R has, behind the scenes, estimated the entire posterior probability distribution for $\beta_0$. We will graph that distribution in the next section. But the basic print method for these objects can't show the entire distribution, so it gives us summary numbers: the median and the MAD SD. Speaking roughly, we would expect about 95% of the values in the posterior to be within two MAD SD's of the median.

As we can see with the able above are using log(OR) which means that the way that we interpret the data given to us plays a huge role in the way that we can predict future data. When we are dealing with income for example we can either use the log of income or deal with income straight which play different roles in changing the way that we feed and understand the data. With the different ways that we are inputting the data, the model changes because of the variables that are changing within the model. 

In summary: we model the income of individuals at a train station as a linear function of the treatment each individual receives. We find that the individuals that receive any Spanish speaking treatment are about half a standard deviation more in attitude than the attitude for individuals that did not receive Spanish speaking treatment meaning that the individuals that received Spanish speaking treatment ended up being less liberal.

### Temperance

```{r}
#| echo: false
#| fig.cap: Temperance
knitr::include_graphics("other/images/Temperance.jpg")
```

#### The Question


#### The Answer


#### Humility


**Courage** gave us the fitted model. With **Temperance** the decisions made with flawed posteriors are almost always better than decisions made without them.

*Among all people who have an income of \$100,000, what proportion are liberal?*

Although our model is now logistic, all the steps in answering a question like this are the same as with a linear/guassian model.

```{r}
newobs <- tibble(income = 100000)

pe <- posterior_epred(fit_4, 
                      newdata = newobs) |> 
  as_tibble()
```

`pe` is a tibble with a single vector. That vector is 4,000 draws from the posterior distribution of proportion of people, among those who make \$100,000, who are liberal. The population proportion is the same thing as the probability for any single individual.

```{r}
pe |> 
  ggplot(aes(x = `1`)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100)  +
    labs(title = "Posterior for Proportion Liberal Among $100,000 Earners",
         subtitle = "The population proportion is the same as the probability for any individual",
         x = "Income",
         y = "Probability of Being Liberal") +
    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```

<!-- Mini discussion of graph? -->

*Assume we have a group of eight people, two of whom make \$100,000, two \$200,000, two \$300,000 and two \$400,000. How many will be liberal?*

Because we are trying to predict the outcome for a small number of units, we use `posterior_predict()`. The more complex the questions we ask, the more care we need to devote to making the `newobs` tibble. We use the same `rowwise()` and `c_across()` tricks as earlier in the chapter.

```{r}
newobs <- tibble(income = c(rep(100000, 2),
                            rep(200000, 2),
                            rep(300000, 2),
                            rep(400000, 2)))
                 

pp <- posterior_predict(fit_4, 
                        newdata = newobs) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across()))

pp
```

Study the `pp` tibble. Understand its component parts. The first column, for example, is 4,000 draws from the posterior distribution for the liberal status of a random person with an income of \$100,000. Note how all the draws are zeroes or ones. That is very different from the draws we have seen before! But it also makes sense. We are making a prediction about a binary variable, a variable which only have two possible values: zero or one. So, any (reasonable!) predictions will only be zero or one.

The second column is the same thing as the first column. Both are 4,000 draws from the posterior distribution for the liberal status of a random person with an income of \$100,000. Yet they also have different values. They are both the same thing and different things, in the same way that `rnorm(10)` and `rnorm(10)` are the same thing --- both are 10 draws from the standard normal distribution --- and different things in that the values vary.

The third and fourth columns are different from the first two columns. They are both 4,000 draws from the posterior distribution for the liberal status of a random person with an income of \$200,000. And so on for later columns. We can answer very difficult questions by putting together simple building blocks, each of them a set of draws from a posterior distribution. Recall the discussion in @sec-distributions.

The `total` column is simply the sum of the first eight columns. Having created the building blocks with 8 columns of draws from four different posterior distributions, we can switch our focus to each row. Consider row 2. It has a vector of 8 numbers: `1 1 1 0 0 1 0 0`. We can treat that vector as a unit of analysis. This is what might happen with our 8 people. The first three might be liberal, the fourth not liberal and so on. This row is just one example of what might happen, one draw from the posterior distribution of possible outcomes for groups of eight people with these incomes.

We can simplify this draw by taking the sum, or doing anything else which might answer the question with which we are confronted. Posterior distributions are as flexible as individual numbers. We can, more or less, just use algebra to work with them.

Graphically we have:

```{r}
pp |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100)  +
    labs(title = "Posterior for Number of Liberals in Group with Varied Incomes",
         subtitle = "Two is the most likely number, but values from 0 to 5 are plausible",
         x = "Number of Liberals",
         y = "Probability") +
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```

As always, there is some *truth*. If, tomorrow, we were to meet 8 new people, with the specified incomes, a certain number of them would be liberal. If we had the ideal Preceptor Table, we could just look up that number. No data science required. Alas, we don't know the truth. The bets we can do is to create a posterior distribution for that unknown value, as we have done here. We then need to translate that posterior into English --- "The most likely number of liberals is 2 or 3, but a total as low as zero or as high as 5 is also plausible. Having 6 liberals would be really surprising. Having 7 or 8 is almost impossible."

Are these two posterior probability distributions perfect? No! This is the central message of the virtue of Temperance. We must demonstrate our humility when we use our models. Recall the distinction between the *unknown true distribution* and the *estimated distribution*. The first is the posterior distribution we would create of we understood every detail of the process and could accurately model it. We would still not know the true unknown number, but our posterior distribution for that number would be perfect. Yet, our model is never perfect. We are making all sorts of assumptions behind the scenes. Some of those assumptions are plausible. Others are less so. Either way, the *estimated distribution* is what we have graphed above. When we take a look at the assumption of validity a possibility may not hold due would be from how our income data was collected similar to our age data collection. When we take a look at the assumption of stability a possibility where it may not hold would be inflation and how that could impact income. When we take a look at the assumption of representativeness a possibility where it may not hold would be with correlation between our "sampling mechanism" and other variables to be true. When we take a look at the assumption of the model structure a possibility where it may not hold would be with the flaw of the fitted model where we conducted a posterior predict check that generated a distribution of values that showed the flaws within the model.

The central lesson of Temperance is: Don't confuse the *estimated* posterior (which is what you have) with the *true* posterior (which is what you want). Recognize the unavoidable imperfections in the process. You can still use your estimated posterior --- what choice do you have? --- but be cautious and humble in doing so. 

The more that you suspect that your estimated posterior differs from the true posterior, the more humble and cautious you should be. Stay cautious my friends.

<!-- Insert meme! -->

## Summary

Throughout this chapter, we explored relationships between different variables in the `trains` data set. We built two predictive models and two causal models.

Similar to previous chapters, our first task is to always use **Wisdom**. We want to judge how relevant our data is based on the questions we ask. Is it reasonable to consider the data we have (e.g., income and age data from Boston commuters in 2012) are being drawn from the same population as the data we want to have (e.g., income and age data from today for the entire US)? Probably? Recall that these questions will bring up the **assumption of validity** and whether our data will be able to "stack" up with each other.

Our next step is to take a look at **Justice** which helps us decide the best way to represent the models that we will make. A little math won't kill you. Translating that math into code will be through the help of **Courage**. Our primary goal is to generate posterior distributions for the parameters and understand/interpret their meaning. Finally, we end off with **Temperance** that uses our models to answer the questions that we have asked above. Remember it is important to hone in on our questions to the most that we can because the most detailed questions will allow for the more accurate answers to prevail.

*Key Lessons and Commands That Were Talked About:*

-   Create a model using `stan_glm()`.

-   Use `posterior_epred()` to estimate expected values. The **e** in **e**pred stands for **e**xpected.

-   Use `posterior_predict()` to make forecasts for individuals. The variable in predictions is always greater than the variability in expectations because predictions can't pretend that $\epsilon_i$ is zero.

-   Once we have draws from a posterior distribution for our outcome variable --- whether that be an expectation or a prediction --- we can manipulate those draws to answer our question.

*Always Remember the Following:*

-   Always explore your data.

-   Predictive models care little about causality.

-   Predictive models and causal models use the same math and the same code.

-   "When comparing" is a great phrase to start the summary of any non-causal model.

-   Don't confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Be cautious in your use of the posterior.

```{r}
#| echo: false
#| cache: false
#| warning: false
knitr::write_bib(.packages(), "packages.bib")
```
