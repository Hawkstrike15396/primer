# Four Parameters {#sec-four-parameters}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(tidyverse)
library(primer.data)
library(gt)
library(patchwork)
```

<!-- This chapter is left-over after removing all the transformation stuff to the mechanics chapter. Turn it into a full scale chapter: four parameters continuous. 

Explore more than one family. Start with gaussian as usual. Then try poisson. Maybe some other positive only distributions. Maybe the special roll-your-own dependent variable.

This will be a predictive model, but it will explore the use of the treatment. Sometimes, we just treat a treatment like any other covariate.

There are several different four parameter models that you could make here. Perhaps look at three, and pick one?

Should we continue to use trains or switch to a different data set? The more I look at att_end as an outcome variable, the less I like it. 

Think of this as the chapter which comes directly after mechanics? And tries to illustrate some points?
-->


<!-- *Our forecasts are more uncertain that a naive use of our models would suggest.* -->

The questions we are asked are always more complex than they appear. Consider:

> *What are the odds that at least 3 out of 5 people will have a conservative attitude toward immigration given that they have all been exposed to Spanish-speakers?*

This question seems straightforward. We simply build a model and then use our favorite tools to create a posterior distribution which will answer the question. But such simplicity depends on an entire set of unstated assumptions about precisely what the words in the question mean. We need to unpack those assumptions step-by-step. The Cardinal Virtues show us the way.


## Wisdom

```{r}
#| echo: false
#| fig.cap: Wisdom
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.

### Preceptor Table

To create the Preceptor Table we first determine its components.

*Causal or predictive model*: At first glance, this model might appear *causal*. After all, the data involving exposure to Spanish-speakers comes from a randomized experiment. But look closer. We are not interested in a causal effect. We don't care about the *difference* between two potential outcomes. Instead, we care only about a person's attitude toward immigration conditional on receiving the treament. We don't care what their attitude would have been if, counter-factually, they had been assigned to control. There is only on outcome column in the Preceptor Table, so the model is *predictive*.

*Units*: The units are individual people. However, the question does not make clear which people we care about. We should go back to our client/colleague/boss and clarify. Let's assume that she is interested in attitudes for adults living in Canada.

*Outcome*: The outcome involves attitudes toward immigration but, apparently, in a binary manner. A person either has a "conservative" attitude toward immigration or he does not. Again, further conversation with our client would be useful, especially once we start looking at the data we have available. 

*Treatment*: Because this is not a causal model, there is no reason to refer to any of the specific covariates as a "treatment." Again, a treatment is simply a covariate that we could, at least in theory, vary in order to answer the question. In this case, we don't care about the effect of the treatment. We are simply building a predictive model for the subset of the population who have been exposed to Spanish-speakers. 

*Covariates*: Besides exposure to Spanish-speakers, it is not clear what other covariates might be useful in answering the question. We need to explore our data set to see what we have available. (Hint: We have a variable about political orientation, `liberal`,  which will prove useful.)

*Moment in Time*: The question does not indicate *when* these attitudes will be measured. Time for more conversation. Assume that the client is interested in the present day, 2024.

Putting these all together, the Preceptor Table might look something like:

```{r}
#| echo: false
tibble(ID = c("1", "2", "...", "10", "11", "...", "N"),
       conservative = c("TRUE", "FALSE", "...", "TRUE", "FALSE", "...", "TRUE"),
       treated = c("TRUE", "FALSE", "...", "FALSE", "TRUE", "...", "TRUE"),
       liberal = c("FALSE", "TRUE", "...", "FALSE", "TRUE", "...", "TRUE"),
       age = c("63", "47", "...", "52", "75", "...", "68"),
       sex = c("Female", "Male", "...", "Female", "Female", "...", "Male")) |>
  gt() |>
  tab_header(title = "Preceptor Table") |>
  cols_label(ID = md("ID"),
             conservative = md("Conservative"),
             treated = md("Treated"),
             liberal = md("Liberal"),
             age = md("Age"),
             sex = md("Sex")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(conservative)) |>
  tab_spanner(label = "Covariates", columns = c(sex, treated, liberal, age))
```

Now that our problems are getting more complex, we allow a little more latitude in the initial design of the Preceptor Table. In the vast majority of real world situtations, it is impossible to fully specify the Preceptor Table at the start of the investigation. It, like the question, evolve over the course of the analysis. In particular, the set of covariates required depends on (changes in) the question(s) being answered.

Recall the definition of a Preceptor Table: "the smallest possible table of data with rows and columns such that, if there is no missing data, then it is easy to calculate the quantities of interest." The initial version of our question only specifies two variables: an attitude toward immigration (the outcome) and exposure to Spanish-speakers (a covariate which really just specifies the units of interest). In other words, the *true* Preceptor Table for the *initial* version of the question really looks like this:

```{r}
#| echo: false
tibble(ID = c("1", "2", "...", "10", "11", "...", "N"),
       conservative = c("TRUE", "FALSE", "...", "TRUE", "FALSE", "...", "TRUE"),
       treated = c("TRUE", "FALSE", "...", "FALSE", "TRUE", "...", "TRUE"),
       liberal = c("FALSE", "TRUE", "...", "FALSE", "TRUE", "...", "TRUE"),
       age = c("63", "47", "...", "52", "75", "...", "68"),
       sex = c("Female", "Male", "...", "Female", "Female", "...", "Male")) |>
  filter(! ID %in% c("2", "10")) |> 
  select(ID, conservative) |> 
  gt() |>
  tab_header(title = "Preceptor Table") |>
  cols_label(ID = md("ID"),
             conservative = md("Conservative")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(conservative))
```

Note:

<!-- DK: Clean/expand? -->

* The rows for ID "2" and "10" have been removed. Those individuals did not receive the treatment, so we don't really care what their attitude toward immigration is. (This is a subtle, and contestable, point which we will return to in later chapters. Understanding which rows you "really" need in your Preceptor Table, or your Population Table, is non-trivial.)

* There are no rows for covariates because the question does not specify them. The question does mention exposure to Spanish-speakers, but that just specifies the relevant universe. We are, by the terms of the question, uninterested in immigration attitudes among people who were not exposed to Spanish-speakers.

* We have one row for every adult in Canada in 2024 who is exposed to Spanish-speakers. Of course, before we define precisely what we mean by such an exposure and/or before we actually cause that exposure, we are not certain which people really qualify for inclusion.

* The table would allow us, via the magic of simulation, to answer our question, and any similar one. Just draw, at random, 5 people from the table and determine whether or not (at least) three are conservative. Do that 1,000 times. The proportion of the draws which have (at least) three conservatives is a good estimate of the odds of getting (at least) three conservatives.

In some sense, the "true" Preceptor Table would just have 5 rows, one for each of the people being referred to in the original question:

```{r}
#| echo: false
tibble(ID = c("63", "122", "3,987", "465,234", "8,001,654"),
       conservative = c("TRUE", "FALSE", "TRUE", "FALSE", "TRUE")) |>
  gt() |>
  tab_header(title = "Preceptor Table") |>
  cols_label(ID = md("ID"),
             conservative = md("Conservative")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(conservative))
```

If we knew the exact five people, all Canadian adults in 2024 who were exposed to Spanish-speakers, and we knew whether or not they were "conservative," then we could answer the question easily. Note that the answer is not 60%, even though 60% (3 out of 5) of these individuals are conservative. The answer to the question is 100% because we know the exact individuals in the group of 5 and we know that 3 of them are, in fact, conservative on immigration.

Of course, such a Preceptor Table, even though it makes answering our question maximally easy, is a fantasy. A good Preceptor Table is not just "the smallest possible table of data with rows and columns such that, if there is no missing data, then it is easy to calculate the quantities of interest," even though that is our (basic) definition. It needs to have some connection to the real world, some tether to reality, which makes the connection to the Population Table more sensible.


### EDA 

We have explored the `trains` tibble from the **primer.data** package in previous chapters. But you can never look at your data too much! So, let's explore some more:


```{r}
#| code-fold: false
trains |> 
  select(att_end, att_start, treatment, liberal, age, sex) |> 
  summary()
```

Recall that `att_end` is a measure of a person's attitude toward immigration, conducted after the experiment. It is on an integer scale, with values ranging from 3 to 15, with higher values corresponding to more conservative views. `att_start` is the same thing, but measured before the experiment.


```{r}
ggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +
  geom_point() +
  labs(title = "Attitude End Compared with Attitude Start and Liberal",
       x = "Attitude at Start of Study",
       y = "Attitude at End of Study",
       color = "Liberal?")
```

Is that data believable? Maybe? One could imagine that `att_end` would be predicted fairly well by `att_start`. This makes sense for most of our data points, which show not much difference between the attitudes. But what about the great disparities in attitude shown for the individual with a starting attitude of 9 and an ending attitude around 15? In a real data science project, this would require further investigation. For now, we ignore the issue and blithely press on.

### Validity

*Validity* is the consistency, or lack thereof, in the columns of your data set and the corresponding columns in your Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a *valid correspondence* with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the *Population Table*, which is the first step in Justice.

The original question refers to "conservative attitude toward immigration" without any clear discussion about what that phrase means. There are many ways to measure a person's attitude toward immigration. There are many possible definitions of "conservative" which might be applied to these measures. For the assumption of *validity* to hold, the relevant variable in our data must be close enough to the variable descripion implicit in our question.

The biggest problem is that `att_end` is a (mostly) continuous variable, measured on a 3 - 15 scale. The question, and the Preceptor Table we created from it, imply a TRUE/FALSE measure as to whether a person has a conservative attitude or does not have a conservative attitude. 

Given that contradiction, we have three options. First, we can give up. *The major part of Wisdom is deciding what questions you can't answer because of the data you don't have.* In this case, however, that seems a bit extreme. The data we have and the data we want are close enough that just a little adjustment will allow the assumption of validity to hold.

Second, we can adjust our question. Instead of asking about a TRUE/FALSE state, we can ponder:

> *What are the odds that at least 3 out of 5 people will have a conservative attitude toward immigration (meaning a value of 10 or more on a 3-15 scale) given that they have all been exposed to Spanish-speakers?*

Changing the question is not cheating. It is a natural part of the process by which we perform data analysis.

Third, we can adjust our data. Again, this is not cheating, as long as are explicit and transparent. Create a new variable, `att_conservative` which is TRUE of `att_end >= 10` and FALSE otherwise. In practice, changing the data and changing the question generally result in similar answers, as they would here. The important thing is to check for *validity*, to ensure that columns in our Preceptor Table and the columns in our data have a *valid correspondence* with one another. This applies, not just to the outcome variable, but to any covariates.

<!-- DK: Explore some of the covariates, at least exposure to Spanish speakers? -->


## Justice 

```{r}
#| echo: false
#| fig.cap: Justice
knitr::include_graphics("other/images/Justice.jpg")
```

*Justice* is the second [Cardinal Virtue](https://en.wikipedia.org/wiki/Cardinal_virtues) in data science. Justice starts with the Population Table – the data we want to have, the data which we actually have, and all the other data from that same population. Each row of the Population Table is defined by a unique unit/time combination. We explore three key issues. First, does the relationship among the variables demonstrate *stability*, meaning is the model stable across different time periods? Second, are the rows associated with the data and, separately, the rows associated with the Preceptor Table *representative* of all the units from the population? Third, for causal models only, we consider *unconfoundedness*

### Population Table

*The population is not the set of units for which we have data.* That is the data set. The population is the larger --- potentially much larger --- set of units about whom we want to make inferences. *The parameters in our models refer to the population, not to the data set.*





### Stability

### Representativeness

### Unconfoundedness

Now that we have considered the connection between our data and the predictions we seek to make, we will need to consider our model. 

First: is our model causal or predictive? Recall that our model measures `att_end` as a function of `liberal` and `att_start`. The variables `liberal` and `att_start` do not involve a control or treatment dynamic. There is no manipulation with these variables. Given that **there is no causation without manipulation**, this is a predictive model. 

We are making inferences about groups of people according to their political affiliation and starting attitude. We are not measuring causality, but we *are* predicting outcomes. 

When creating a parallel slops model, we use the basic equation of a line: 

$$y_i = \beta_0  + \beta_1 x_{1,i} + \beta_2 x_{2,i}$$

If $y = att\_end$, $x_1 = att\_start$, $x\_2 = liberal$, then the equations are as follows:

If liberal = FALSE:

$$y_i = \beta_0  + \beta_1 x_{1,i}$$
Which equates to, in `y = b + mx` form:

$$y_i = intercept +  \beta_1 att\_start_i$$

If liberal = TRUE: 

$$y_i = (\beta_0  + \beta_2) + \beta_1 x_{1,i}$$

Which equates to, in `y = b + mx` form:

$$y_i = (intercept + liberal\_true) + \beta_1 att\_start_i$$

<!-- DK: Explain, slowly, how we interpret the coefficients, and combinations of the coefficients in the case with treatment and liberal. The intercept is the expected value for someone with FALSE for both treatment and liberal.  The expected value for someone who is both liberal and got the treatment is b_0 + b_1 + b_2. -->

## Courage

```{r}
#| echo: false
#| fig.cap: Courage
knitr::include_graphics("other/images/Courage.jpg")
``` 

Justice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.

### Models



The use of `stan_glm` is the same as usual. Using `stan_glm`, we will create our model, `fit_1`. 

```{r}
fit_1 <- stan_glm(formula = att_end ~ liberal + att_start,
                  data = trains,
                  refresh = 0,
                  seed = 42)

fit_1
```


To remind ourselves, recall that the `(Intercept)` here is representing the `att_end` value for cases where `liberal = FALSE` and `treatment does not equal Control`. The next row, `liberalTRUE` gives a median value which represents the *offset* in the prediction compared with the `(Intercept)`. In other words, the true intercept for cases where `liberal = TRUE` is represented by $(Intercept) + liberalTRUE$. The value of `treatmentControl` is the offset in `att_end` for those in the Control group. 

To find our intercepts, we will need to tidy our regression using the `tidy()` function from the **broom.mixed** package. We tidy our data and extract values to create a parallel slopes model. The parallel slopes model allows us to visualize multi-variate Bayesian modeling (i.e. modeling with more than one explanatory variable). That is a complicated way of saying that we will visualize the fitted model created above in a way that allows us to see the intercepts and slopes for two different groups, `liberalTRUE` and `liberalFALSE`: 

```{r}
# First, we will tidy the data from our model and select the term and estimate.
# This allows us to create our regression lines more easily.

tidy <- fit_1 |> 
  tidy() |> 
  select(term, estimate)

tidy

# Extract and name the columns of our tidy object. By calling tidy$estimate[1],
# we are telling R to extract the first value from the estimate column in our
# tidy object.

intercept <- tidy$estimate[1]
liberal_true <- tidy$estimate[2]
att_start <- tidy$estimate[3]

```

Now, we can define the following terms—- `liberal_false_intercept` and `liberal_false_att_slope`; and `liberal_true_intercept` and `liberal_true_att_slope`:


```{r}
# Recall that the (Intercept) shows us the estimate for the case where liberal =
# FALSE. We want to extract the liberal_false_intercept to indicate where the
# intercept in our visualization should be. The slope for this case, and for the
# liberal = TRUE case, is att_start.

liberal_false_intercept <- intercept
liberal_false_att_slope <- att_start

#  When wanting the intercept for liberal = TRUE, recall that the estimate for
#  liberalTRUE is the offset from our (Intercept). Therefore, to know the true
#  intercept, we must add liberal_true to our intercept.

liberal_true_intercept <- intercept + liberal_true
liberal_true_att_slope <- att_start
```

All we've done here is extracted the values for our intercepts and slopes, and named them to be separated into two groups. This allows us to create a `geom_abline` object that takes a unique slope and intercept value, so we can separate the `liberalTRUE` and `liberalFALSE` observations. 

```{r}
# From the data set trains, use att_start for the x-axis and att_end for
# the y-axis with color as liberal. This will split our data into two color
# coordinates (one for liberal = TRUE and one for liberal = FALSE)

ggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +
  
  # Use geom_point to show the datapoints. 
  
  geom_point() +
  
  # Create a geom_abline object for the liberal false values. Set the intercept
  # equal to our previously created liberal_false_intercept, while setting slope
  # equal to our previously created liberal_false_att_slope. The color call is
  # for coral, to match the colors used by tidyverse for geom_point().
  
  geom_abline(intercept = liberal_false_intercept,
              slope = liberal_false_att_slope, 
              color = "#F8766D", 
              size = 1) +
  
  # Create a geom_abline object for the liberal TRUE values. Set the intercept
  # equal to our previously created liberal_true_intercept, while setting slope
  # equal to our previously created liberal_true_att_slope. The color call is
  # for teal, to match the colors used by tidyverse for geom_point().

  geom_abline(intercept = liberal_true_intercept,
              slope = liberal_true_att_slope,
              color = "#00BFC4", 
              size = 1) +
  
  # Add the appropriate titles and axis labels. 
  
  labs(title = "Parallel Slopes Model",
       x = "Attitude at Start", 
       y = "Attitude at End", 
       color = "Liberal") 
```

This is our parallel slopes model. What we have done, essentially, is created a unique line for `liberalTRUE` and `liberalFALSE` to observe the differences in the groups as related to attitude start and attitude end. 

As we can see, commuters who are *not* liberal tend to start with slightly higher values for `att_start`. Commuters who *are* liberal tend to have lower starting values for `att_start`. 

Now, what if we want to look at another model? To judge whether we can have a superior model? Let's create another object, using `stan_glm`, that looks at `att_end` as a function of `treatment` and `att_start`. 


```{r}
fit_2 <- stan_glm(formula = att_end ~ treatment + att_start,
                  data = trains,
                  refresh = 0,
                  seed = 56)

fit_2
```

To interpret briefly: 

* `(Intercept)` here is representing the `att_end` value for cases where `treatment does not equal Control`. 

* `treamtmentControl` gives a median value which represents the *offset* in the prediction compared with the `(Intercept)`. In other words, the true intercept for cases where `treatment = Control` is represented by $(Intercept) + treatmentControl$. 

* `att_start` represents the slope for both groups representing a unit change. 

An important point here is that **these models are causal**. When including the variable `treatment`, we have a measured causal effect of a condition. This is different from our prior parallel slopes model, where we were modeling for prediction, not causation. 


<!-- DK: A causal model is one in which, at least in theory, one of the righthand side variables could have been manipulated, and when you care about estimating causal effects. If someone just needs to forecast or predict something that, almost always, she is not asking a causal question, so she does not care if the righthand side variables could be manipulated. Is the sunset beautiful if no one is looking at it? Is the model causal if I just want to predict? -->

To see how these models compare in performance, we will perform leave-one-out (LOO) cross validation again. 


```{r}
L1 <- loo(fit_1)

L1
```
Perform `loo()` on our second model:

```{r}
L2 <- loo(fit_2)

L2
```

Recall that the relevant data is the data from `elpd_loo`. The estimates for `elpd_loo` vary quite a bit. Recall that the superior model will have a value of `elpd_loo()` that is closer to 0. The standard error (SE) for these models also differs some. To compare these directly, we will use `loo_compare`.  

```{r}
loo_compare(L1, L2)
```

Recall that, with `loo_compare()`, the resulting data shows the superior model first, with values of 0 for `elpd_diff` and `se_diff`, since it compares the models to the best option. The values of `elpd_diff` and `se_diff` for `fit_1` show the difference in the models. As we can see, `fit_2`, the model which looks at `treatment` + `att_start`, is better. 

But how certain can we be that it is better? Note that the difference between the two models is not quite two standard errors. So, there is a reasonable possibility that the difference is due to chance.

<!-- DK: Say more here! -->

### Tests

### Data Generating Mechanism



## Temperance

What we really care about is data we haven't seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn't change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that are forecasts are more uncertain that a naive use of our model might suggest. 
Having created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.

Preceptor's Posterior is the posterior you would calculate if all the assumptions you made under Wisdom and Justice were correct. Sadly, they never are! So, you can never know Preceptor's Posterior. Our posterior will, we hope, be a close-ish approximation of Preceptor's Posterior.


### Testing is nonsense 

As always, it is important to look at the practices of other professionals and the reasons we may choose not to follow those tactics. For instance, our continued problem with hypothesis testing. In hypothesis testing, we assert a null hypothesis $H_0$ about our data and an alternative hypothesis $H_a$. 

When performing hypothesis testing, we either reject the hypothesis or we do not reject it. The qualifications for rejecting are met if the 95% confidence interval excludes the null hypothesis. If the hypothesis *is* included in our 95% confidence interval, we do not reject it. In the case of “insignificant” results, with p > 0.5, we also can’t "reject" the null hypothesis. However, this does not mean that we accept it.

The premise of hypothesis testing is to answer a specific question -- one that may not even be particularly relevant to our understanding of the world -- about our data. So, what are our problems with hypothesis testing? 
- Rejecting or not rejecting hypotheses doesn't helps us to answer real questions.
- The fact that a difference is not “significant” has no relevance to how we use the posterior to make decisions.
- Statistical significance is not equal to *practical* importance. 
- There is no reason to **test** when you can summarize by providing the full posterior probability distribution.


<!-- Discuss the difference in our estimated treatment effect for this chapter and the model in chapter 8 and in the model we did not use here. There is no truth! Which is why we need to be humble and cautious when using our posterior. Be Temperate! -->

<!-- att_end ~ liberal + att_start. Same thing as chapters before. Show graphic of parallel lines. Cardinal virtues as always. And, extra, at the end we can use loo_compare to see if this model is better than simpler model of att_end ~ att_start. -->

<!-- What is the expected difference in attitude end between a liberal with start 6 and a non liberal with start 10? Do that by hand with posterior_epred. Then show how you can get a similar answer just by interpreting coefficents. -->


## Summary

In this chapter, we covered a number of topics important to effectively creating models. 

*Key commands*:
- Create a model using `stan_glm()`. 
- After creating a model, we can use `loo()` to perform leave-one-out cross validation. This assesses how effectively our model makes predictions for data it has not seen yet.
- The command `loo_compare()` allows us to compare two models, to see which one performs better in leave-one-out cross validation. The superior model makes better predictions. 

*Remember*: 
- We can transform variables -- through centering, scaling, taking logs, etc. -- to make them more sensible. Consider using a transformation if the intercept is awkward. For instance, if the intercept for `age` represents the estimate for people of age zero, we might consider transforming `age` to be easier to interpret. 
- When selecting variables to include in our model, follow this rule: keep it if the variable has a large and well-estimated coefficient. This means that the 95% confidence interval excludes zero. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.
- When we compare *across unit*, meaning comparing Joe and George, we are not looking at a causal relationship. *Within unit* discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying **one unit** under **two conditions**. 
- When we talk about two potential outcomes, we are discussing the same person or unit under two conditions. 

```{r}
#| echo: false
#| cache: false
#| warning: false
knitr::write_bib(.packages(), "packages.bib")
```

