# Four Parameters {#sec-four-parameters}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(patchwork)
```

<!-- This chapter is left-over after removing all the transformation stuff to the mechanics chapter. Not sure what to do with it. -->


*Our forecasts are more uncertain that a naive use of our models would suggest.*

In our haste to make progress --- to get all the way through the process of building, interpreting and using models --- we have given short shrift to some of the messy details of model building and evaluation. This chapter fills in those lacunae. We will also introduce models with four parameters, including *parallel slopes* models. 



To conclude this chapter, we will look at a four parameter model. The model we will use will measure `att_end` as a function of `liberal` or `att_start` or `treatment` or some combination of these variables. 

<!-- DK: We are no longer in the business of telling you what the model should be. There are lots of models we could use! We need to explore the different models and then pick one. -->

<!-- DK: There are three different four parameter models that you could make here. Let's look at all three, pick one and then use it. -->

## Wisdom

```{r}
#| echo: false
#| fig.cap: Wisdom
knitr::include_graphics("other/images/Wisdom.jpg")
```


<!-- ### Preceptor Table -->

<!-- *Causal or predictive model*: -->

<!-- *Units*: -->

<!-- *Outcome*: -->

<!-- *Treatment*: -->

<!-- *Covariates*: -->

<!-- *Moment in Time*:  -->

Before making a model which seeks to explain `att_end`, we should plot it and the variables we think are connected to it:

```{r}
ggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +
  geom_point() +
  labs(title = "Attitude End Compared with Attitude Start and Liberal",
       x = "Attitude at Start of Study",
       y = "Attitude at End of Study",
       color = "Liberal?")
```

Is that data believable? Maybe? One could imagine that `att_end` would be predicted fairly well by `att_start`. This makes sense for most of our data points, which show not much difference between the attitudes. But what about the great disparities in attitude shown for the individual with a starting attitude of 9 and an ending attitude around 15? In a real data science project, this would require further investigation. For now, we ignore the issue and blithely press on.

Another component of Wisdom is the population. The concept of the "population" is subtle and important. *The population is not the set of commuters for which we have data.* That is the dataset. The population is the larger --- potentially much larger --- set of individuals about whom we want to make inferences. *The parameters in our models refer to the population, not to the dataset.*

There are many different populations, each with its own $\mu$, in which we might be interested. For instance:

* The population of Boston commuters on the specific train and time included in our dataset. 

* The population of *all* Boston commuters.

* The population of commuters in the United States. 

All of these populations are different, so each has a different $\mu$. Which $\mu$ we are interested in depends on the problem we are trying to solve. It is a judgment call, a matter of Wisdom, as to whether or not that data we have is "close enough" to the population we are interested in to justify making a model. 

**The major part of Wisdom is deciding what questions you can't answer because of the data you don't have.**

## Justice 

```{r}
#| echo: false
#| fig.cap: Justice
knitr::include_graphics("other/images/Justice.jpg")
```

Now that we have considered the connection between our data and the predictions we seek to make, we will need to consider our model. 

First: is our model causal or predictive? Recall that our model measures `att_end` as a function of `liberal` and `att_start`. The variables `liberal` and `att_start` do not involve a control or treatment dynamic. There is no manipulation with these variables. Given that **there is no causation without manipulation**, this is a predictive model. 

We are making inferences about groups of people according to their political affiliation and starting attitude. We are not measuring causality, but we *are* predicting outcomes. 

When creating a parallel slops model, we use the basic equation of a line: 

$$y_i = \beta_0  + \beta_1 x_{1,i} + \beta_2 x_{2,i}$$

If $y = att\_end$, $x_1 = att\_start$, $x\_2 = liberal$, then the equations are as follows:

If liberal = FALSE:

$$y_i = \beta_0  + \beta_1 x_{1,i}$$
Which equates to, in `y = b + mx` form:

$$y_i = intercept +  \beta_1 att\_start_i$$

If liberal = TRUE: 

$$y_i = (\beta_0  + \beta_2) + \beta_1 x_{1,i}$$

Which equates to, in `y = b + mx` form:

$$y_i = (intercept + liberal\_true) + \beta_1 att\_start_i$$

<!-- DK: Explain, slowly, how we interpret the coefficients, and combinations of the coefficients in the case with treatment and liberal. The intercept is the expected value for someone with FALSE for both treatment and liberal.  The expected value for someone who is both liberal and got the treatment is b_0 + b_1 + b_2. -->

## Courage

```{r}
#| echo: false
#| fig.cap: Courage
knitr::include_graphics("other/images/Courage.jpg")
``` 

The use of `stan_glm` is the same as usual. Using `stan_glm`, we will create our model, `fit_1`. 

```{r}
fit_1 <- stan_glm(formula = att_end ~ liberal + att_start,
                  data = trains,
                  refresh = 0,
                  seed = 42)

fit_1
```


To remind ourselves, recall that the `(Intercept)` here is representing the `att_end` value for cases where `liberal = FALSE` and `treatment does not equal Control`. The next row, `liberalTRUE` gives a median value which represents the *offset* in the prediction compared with the `(Intercept)`. In other words, the true intercept for cases where `liberal = TRUE` is represented by $(Intercept) + liberalTRUE$. The value of `treatmentControl` is the offset in `att_end` for those in the Control group. 

To find our intercepts, we will need to tidy our regression using the `tidy()` function from the **broom.mixed** package. We tidy our data and extract values to create a parallel slopes model. The parallel slopes model allows us to visualize multi-variate Bayesian modeling (i.e. modeling with more than one explanatory variable). That is a complicated way of saying that we will visualize the fitted model created above in a way that allows us to see the intercepts and slopes for two different groups, `liberalTRUE` and `liberalFALSE`: 

```{r}
# First, we will tidy the data from our model and select the term and estimate.
# This allows us to create our regression lines more easily.

tidy <- fit_1 |> 
  tidy() |> 
  select(term, estimate)

tidy

# Extract and name the columns of our tidy object. By calling tidy$estimate[1],
# we are telling R to extract the first value from the estimate column in our
# tidy object.

intercept <- tidy$estimate[1]
liberal_true <- tidy$estimate[2]
att_start <- tidy$estimate[3]

```

Now, we can define the following terms—- `liberal_false_intercept` and `liberal_false_att_slope`; and `liberal_true_intercept` and `liberal_true_att_slope`:


```{r}
# Recall that the (Intercept) shows us the estimate for the case where liberal =
# FALSE. We want to extract the liberal_false_intercept to indicate where the
# intercept in our visualization should be. The slope for this case, and for the
# liberal = TRUE case, is att_start.

liberal_false_intercept <- intercept
liberal_false_att_slope <- att_start

#  When wanting the intercept for liberal = TRUE, recall that the estimate for
#  liberalTRUE is the offset from our (Intercept). Therefore, to know the true
#  intercept, we must add liberal_true to our intercept.

liberal_true_intercept <- intercept + liberal_true
liberal_true_att_slope <- att_start
```

All we've done here is extracted the values for our intercepts and slopes, and named them to be separated into two groups. This allows us to create a `geom_abline` object that takes a unique slope and intercept value, so we can separate the `liberalTRUE` and `liberalFALSE` observations. 

```{r}
# From the dataset trains, use att_start for the x-axis and att_end for
# the y-axis with color as liberal. This will split our data into two color
# coordinates (one for liberal = TRUE and one for liberal = FALSE)

ggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +
  
  # Use geom_point to show the datapoints. 
  
  geom_point() +
  
  # Create a geom_abline object for the liberal false values. Set the intercept
  # equal to our previously created liberal_false_intercept, while setting slope
  # equal to our previously created liberal_false_att_slope. The color call is
  # for coral, to match the colors used by tidyverse for geom_point().
  
  geom_abline(intercept = liberal_false_intercept,
              slope = liberal_false_att_slope, 
              color = "#F8766D", 
              size = 1) +
  
  # Create a geom_abline object for the liberal TRUE values. Set the intercept
  # equal to our previously created liberal_true_intercept, while setting slope
  # equal to our previously created liberal_true_att_slope. The color call is
  # for teal, to match the colors used by tidyverse for geom_point().

  geom_abline(intercept = liberal_true_intercept,
              slope = liberal_true_att_slope,
              color = "#00BFC4", 
              size = 1) +
  
  # Add the appropriate titles and axis labels. 
  
  labs(title = "Parallel Slopes Model",
       x = "Attitude at Start", 
       y = "Attitude at End", 
       color = "Liberal") 
```

This is our parallel slopes model. What we have done, essentially, is created a unique line for `liberalTRUE` and `liberalFALSE` to observe the differences in the groups as related to attitude start and attitude end. 

As we can see, commuters who are *not* liberal tend to start with slightly higher values for `att_start`. Commuters who *are* liberal tend to have lower starting values for `att_start`. 

Now, what if we want to look at another model? To judge whether we can have a superior model? Let's create another object, using `stan_glm`, that looks at `att_end` as a function of `treatment` and `att_start`. 


```{r}
fit_2 <- stan_glm(formula = att_end ~ treatment + att_start,
                  data = trains,
                  refresh = 0,
                  seed = 56)

fit_2
```

To interpret briefly: 

* `(Intercept)` here is representing the `att_end` value for cases where `treatment does not equal Control`. 

* `treamtmentControl` gives a median value which represents the *offset* in the prediction compared with the `(Intercept)`. In other words, the true intercept for cases where `treatment = Control` is represented by $(Intercept) + treatmentControl$. 

* `att_start` represents the slope for both groups representing a unit change. 

An important point here is that **these models are causal**. When including the variable `treatment`, we have a measured causal effect of a condition. This is different from our prior parallel slopes model, where we were modeling for prediction, not causation. 


<!-- DK: A causal model is one in which, at least in theory, one of the righthand side variables could have been manipulated, and when you care about estimating causal effects. If someone just needs to forecast or predict something that, almost always, she is not asking a causal question, so she does not care if the righthand side variables could be manipulated. Is the sunset beautiful if no one is looking at it? Is the model causal if I just want to predict? -->

To see how these models compare in performance, we will perform leave-one-out (LOO) cross validation again. 


```{r}
L1 <- loo(fit_1)

L1
```
Perform `loo()` on our second model:

```{r}
L2 <- loo(fit_2)

L2
```

Recall that the relevant data is the data from `elpd_loo`. The estimates for `elpd_loo` vary quite a bit. Recall that the superior model will have a value of `elpd_loo()` that is closer to 0. The standard error (SE) for these models also differs some. To compare these directly, we will use `loo_compare`.  

```{r}
loo_compare(L1, L2)
```

Recall that, with `loo_compare()`, the resulting data shows the superior model first, with values of 0 for `elpd_diff` and `se_diff`, since it compares the models to the best option. The values of `elpd_diff` and `se_diff` for `fit_1` show the difference in the models. As we can see, `fit_2`, the model which looks at `treatment` + `att_start`, is better. 

But how certain can we be that it is better? Note that the difference between the two models is not quite two standard errors. So, there is a reasonable possibility that the difference is due to chance.

<!-- DK: Say more here! -->

## Temperance

What we really care about is data we haven't seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn't change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that are forecasts are more uncertain that a naive use of our model might suggest. 
Having created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.

Preceptor's Posterior is the posterior you would calculate if all the assumptions you made under Wisdom and Justice were correct. Sadly, they never are! So, you can never know Preceptor's Posterior. Our posterior will, we hope, be a close-ish approximation of Preceptor's Posterior.


### Testing is nonsense 

As always, it is important to look at the practices of other professionals and the reasons we may choose not to follow those tactics. For instance, our continued problem with hypothesis testing. In hypothesis testing, we assert a null hypothesis $H_0$ about our data and an alternative hypothesis $H_a$. 

When performing hypothesis testing, we either reject the hypothesis or we do not reject it. The qualifications for rejecting are met if the 95% confidence interval excludes the null hypothesis. If the hypothesis *is* included in our 95% confidence interval, we do not reject it. In the case of “insignificant” results, with p > 0.5, we also can’t "reject" the null hypothesis. However, this does not mean that we accept it.

The premise of hypothesis testing is to answer a specific question -- one that may not even be particularly relevant to our understanding of the world -- about our data. So, what are our problems with hypothesis testing? 
- Rejecting or not rejecting hypotheses doesn't helps us to answer real questions.
- The fact that a difference is not “significant” has no relevance to how we use the posterior to make decisions.
- Statistical significance is not equal to *practical* importance. 
- There is no reason to **test** when you can summarize by providing the full posterior probability distribution.


<!-- Discuss the difference in our estimated treatment effect for this chapter and the model in chapter 8 and in the model we did not use here. There is no truth! Which is why we need to be humble and cautious when using our posterior. Be Temperate! -->

<!-- att_end ~ liberal + att_start. Same thing as chapters before. Show graphic of parallel lines. Cardinal virtues as always. And, extra, at the end we can use loo_compare to see if this model is better than simpler model of att_end ~ att_start. -->

<!-- What is the expected difference in attitude end between a liberal with start 6 and a non liberal with start 10? Do that by hand with posterior_epred. Then show how you can get a similar answer just by interpreting coefficents. -->


## Summary

In this chapter, we covered a number of topics important to effectively creating models. 

*Key commands*:
- Create a model using `stan_glm()`. 
- After creating a model, we can use `loo()` to perform leave-one-out cross validation. This assesses how effectively our model makes predictions for data it has not seen yet.
- The command `loo_compare()` allows us to compare two models, to see which one performs better in leave-one-out cross validation. The superior model makes better predictions. 

*Remember*: 
- We can transform variables -- through centering, scaling, taking logs, etc. -- to make them more sensible. Consider using a transformation if the intercept is awkward. For instance, if the intercept for `age` represents the estimate for people of age zero, we might consider transforming `age` to be easier to interpret. 
- When selecting variables to include in our model, follow this rule: keep it if the variable has a large and well-estimated coefficient. This means that the 95% confidence interval excludes zero. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.
- When we compare *across unit*, meaning comparing Joe and George, we are not looking at a causal relationship. *Within unit* discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying **one unit** under **two conditions**. 
- When we talk about two potential outcomes, we are discussing the same person or unit under two conditions. 

```{r}
#| echo: false
#| cache: false
#| warning: false
knitr::write_bib(.packages(), "packages.bib")
```

