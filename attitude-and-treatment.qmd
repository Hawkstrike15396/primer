# Three Parameters {#sec-three-parameters}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(gt)
library(gtsummary)
library(patchwork)
```

<!-- Is the chapter too long? Should I just have a single question for each section? Should I cut out one of the sections entirely? -->

<!-- Standardize years. Data is from 2012. Questions are about 2024. -->

<!-- Remove sampling & assignment mechanism -->

<!-- Come up with common ways in which a lack of representativeness causes problems. This is a list of things that we can check each time when we are interrogating a data science problem. Key issue is the correlation, if anything, between the "assignment mechanism" or "sampling mechanism" and other variables. Discuss sampling mechanism in all cases except att_end ~ treatment. For that, use "assignment mechanism." -->

<!-- Make residual plots consistent in this chapter! Also, we should ensure that the width of 10 units is the same in each plot. Or does that already happen? Want the intuition that, the more predictive the model, the smaller the remaining variation in the residuals.  -->

<!-- Whenever we have a predictive model, use language like "when comparing." Avoid even the hint of a causal claim. -->

Models have parameters. In @sec-sampling we created a model with a single parameter $\rho$, the proportion of red beads in an urn. In @sec-two-parameters, we used models with two parameters: $\mu$ (the average height in the population, generically known as a model "intercept") and $\sigma$ (the variation in height in the population). Here --- can you guess where this is going? --- we will build models with three parameters: $\sigma$ (which serves the same role throughout the book) and two "coefficients." In models which relate a predictor to the outcome, those two parameters will be labeled $\beta_0$ and $\beta_1$. All this notation is confusing, not least because different academic fields use inconsistent schemes. Follow the Cardinal Virtues and tackle your problem step by step.

Perhaps more importantly, a focus on parameters is less relevant now than it was decades ago, when computational limitations made answering our actual questions harder. *Parameters are imaginary.* They don't exist. They, in general, are not the answer to a real world question. They are tools, along with the models of which they are a part, we use to answer questions.

Packages:

```{r}
#| message: false
#| code-fold: false
library(primer.data)
library(tidyverse)
library(brms)
library(tidybayes)
```

We are concerned with these three packages because we are exploring the `trains` dataset which exists within the **primer.data** package. We use the **brms** package to build Bayesian models. ("brms" stands for **B**ayesian **r**egression **m**odel**s**.) The **tidybayes** packages makes working the fitted models easierr. As usual, we use the **tidyverse** package.

In this chapter, we are going to ask a series of questions involving train commuters' ages, party affiliations, incomes, and political ideology, as well as the causal effect of exposure to Spanish-speakers on their attitude toward immigration. These questions will pertain to all train commuters in the US today. For a refresher on this data, refer to @sec-rubin-causal-model.

## att_end \~ treatment

Above, we created a predictive model: with someone's party affiliation, we can make a better guess as to what their age is than we could have in the absence of information about their party. There was nothing causal about that model. Changing someone's party registration can not change their age. In this example, we build a causal model. Consider two questions:

*What is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?*

*What is the largest causal effect which still has a 1 in 10 chance of occurring?*

Answering causal questions requires (at least) two potential outcomes: immigration attitudes when a person receives the treatment of being exposed to Spanish-speakers and immigration attitudes, for that same person, when they don't receive the treatment. We can answer these and similar questions by creating a model with immigration attitude as the dependent variable and exposure to Spanish-speakers as the independent variable. Let's follow the four Cardinal Virtues: Wisdom, Justice, Courage and Temperance.

### Wisdom

```{r}
#| echo: false
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of "validity," as to whether or not we can (reasonably!) assume that the two come from the same population.

#### Preceptor Table

*Causal or predictive model*: In this case, the model is clearly causal, so our Preceptor Table will have two columns for the potential outcomes. If all you need to know to answer the question is the outcome under one value of the treatment, then the model is predictive. 

*Outcome*: A person's attitude toward immigration is the **outcome**. When we take a look at the questions we discover a flaw exists. The individual that asked this question did not tell us where or when we are looking at these questions. It is important to understand where and when that we are answering these questions because otherwise our data could be not accurately applied to the whole country, nation, or specific state. Therefore, by diving deeper within these questions we are able to accurately answer our question with the population that it refers to. For the questions that we have at hand we will be talking about all the adults in July 1, 2012 at the location of Chicago, Illinois which will allow us to answer the questions more accurately. 

*Units*: Our units for this scenario would be individuals because the questions are about the attributes of unique people at the station.

*Treatment* In any causal model, there is at least one covariate which is defined as the "treatment," something which we can manipulate so that some units receive one version and other units get a different version. A "treatment" is just a covariate which we *could* manipulate, at least in theory. In this case, the treatment is exposure to Spanish-speakers. Units can either be exposed, i.e., they receive the "treatment," or they can not be exposed, i.e., they receive the "control." 

Let's look at our refined question to create our Preceptor Table:

*What is the average treatment effect, of exposing people to Spanish-speakers in Chicago Illinois in 2012, on their attitudes toward immigration?*

Our Preceptor Table:

```{r}
#| echo: false
tibble(ID = c("1", "2", "...", "10", "11", "...", "N"),
       attitude_after_control = c("5*", "7", "...", "3*", "10*", "...", "6"),
       attitude_after_treated = c("8", "4*", "...", "5", "7", "...", "13*"),
       treatment = c("Yes", "No", "...", "Yes", "Yes", "...", "No")) |>
  
  gt() |>
  tab_header(title = "Preceptor Table") |> 
  cols_label(ID = md("ID"),
             attitude_after_control = md("Control Ending Attitude"),
             attitude_after_treated = md("Treated Ending Attitude"),
             treatment = md("Treatment")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Potential Outcomes", columns = c(attitude_after_control, attitude_after_treated)) |>
  tab_spanner(label = "Covariate", columns = c(treatment))
```

<!-- DK: Make the explanation of the stars a footnote. -->

Note: the values that have a star next to them symbolize the possible values that may exist if they were "control" instead of "treated" or vice versa.

A Preceptor Table is the smallest possible table with rows and columns such that, if there is no missing data, all our questions are easy to answer. To answer questions --- like "What is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?" or "What is the largest causal effect which still has a 1 in 10 chance of occurring?" --- we need a row for every individual.

#### Exploratory Data Analysis

Recall the discussion from @sec-rubin-causal-model. @enos2014 randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. The data that we want to analyze consists of the attitude toward immigration after the experiment is complete (`att_end`) and the exposure to Spanish-speakers (`treatment`) of each individual on these train platforms. 

```{r}
trains |>
  select(att_end, treatment)
```

The `treatment` can either be "Treated" or "Control" which are the two factors that may influence `att_end`. Participants were asked three questions about immigration issues, each of which allowed for an answer indicated strength of agreement on a scale form 1 to 5, with higher values for `att_end` indicating more agreement with conservative viewpoints.

```{r}
trains |>
  select(att_end, treatment) |>
  summary()
```

The data include information about each respondent's gender, political affiliations, age, income and so on. `treatment` indicates whether a subject was in the control or treatment group. The key outcomes are their attitudes toward immigration both before (`att_start`) and after (`att_end`) the experiment.

`summary()` shows us what the different values of `att_end` and `treatment` are because it is a factor. The range for `att_end` seems reasonable.

```{r}
trains |> 
  ggplot(aes(x = att_end, fill = treatment)) +
    geom_bar(aes(y = after_stat(count/sum(count))),
                   position = "dodge") +
    labs(title = "Ending Attitude Toward Immigration",
         subtitle = "Treated Individuals Are More Conservative",
         x = "Attitude",
         y = "Probability",
         fill = NULL) +
    scale_y_continuous(labels = scales::percent_format()) + 
    theme_classic()
```

We can never know the true average attitude of all treated in the population. But we can calculate a probability distribution for each parameter. As we can see with the plot above, the treated individuals tend to be more conservative. When compared with the controlled population, the treated individuals tend to show more signs of being conservative after having the treatment applied to them.

#### Validity

Now the *assumption of validity* may not hold due to the possibility of when our data was collected. For example if the data was collected in the morning for our Preceptor Table the people that are interviewed may be more grumpy when going to work. Compared to the possibility of our other data being collected at night when the people are more relaxed coming back from work to go home.

Another case where the *assumption of validity* may not hold because the variables may be too different from each other within the data and the Preceptor Table as the columns are not similar enough between each other. One possible reason we may consider is that the Preceptor Table is all the people in Chicago in 2024. However, the data we have involves train commuters in the Boston area in 2012.

While the assumption can prove validity to be wrong, overall, the assumption of validity seems reasonable enough. `att_end` and `treatment` are similar enough between to the underlying concepts in the Preceptor Table and the data that we can "stack" them on top of each other. We can assume that both are drawn from the same population.

### Justice

```{r}
#| echo: false
#| fig.cap: Justice
knitr::include_graphics("other/images/Justice.jpg")
```

The four concerns of Justice remain the same: Population Table, stability, representativeness, and unconfoundedness.

#### Population Table

After assuming validity, we can now create our Population Table. Recall that every row from both the Preceptor Table and the data is included in the Population Table, along with all the rows from the underlying population from which we assume that both the Preceptor Table and the data were drawn. The Population Table includes rows from three sources: the **Preceptor Table**, the **actual data**, and all *other* members of the population.

```{r}
#| echo: false
tibble(source = c("...", "Data", "Data", "...", 
                  "...", "Preceptor Table", "Preceptor Table", "..."),
       att_treat = c("...", "7*", "6", "...",
                     "...", "...", "...", "..."),
       att_control = c("...", "2", "10*", "...", 
                       "...", "...", "...", "..."),
       city = c("...", "Boston, MA", "Boston, MA", "...", 
                "...", "Chicago, IL", "Chicago, IL", "..."),
       year = c("...", "2012", "2012", "...", 
                "...", "2024", "2024", "..."),
       treatment = c("...", "No", "Yes", "...", 
                     "...", "...", "...", "...")) |>
  
  gt() |>
  tab_header(title = "Population Table") |> 
  cols_label(source = md("Source"),
             att_treat = md("Treated"),
             att_control = md("Controlled"),
             treatment = md("Treatment"),
             city = md("City"),
             year = md("Year")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Potential Outcomes", columns = c(att_control, att_treat)) |>
  tab_spanner(label = "Covariates", columns = c(treatment, year, city))
```

Note: the values that have a star next to them symbolize the possible values that may exist if they were "control" instead of "treated" or vice versa.

Our year within the Population table is an example of the moment in time.

Our **actual data** rows contain the information that we *do* know. These rows contain entries for both our covariates *and* the outcomes. In this case, the actual data comes from a study conducted on train commuters around Boston, MA in 2012, so our city entries for these rows will read "Boston, MA" and our year entries of these rows will read "2012".

#### Stability

Consider the **stability** of our model for the relationship between `att_end` and `treatment` between 2012 and 2024. Is this relationship from 2012, four years before Donald Trump's election as president, still the same? While we might not know for sure, we have to consider this in order to continue and make assumptions with our data. For our purposes, we will consider the relationship to be stable. Even though we know that there may have been some changes, we will consider the model to be the same in both years.

#### Representativeness

**Representativeness** has to do with how well our sample represents the larger population we are interested in generalizing to. We might run into two potential problems: Is our data representative of the population? Is our Preceptor Table representative of the population? Let's say that even though Boston is different from Chicago, we considered Boston to be perfectly representative of Chicago. Great, but this 2012 data could still not be representative. This is because there could be bias within those who are chosen to give the survey, in that the commuters who are approached and receive the surveys may not be random or representative. When we deal with representativeness we deal with two primary concerns: the generic concern and the variable concern. In our previous section we talked about the generic concern which applies the same within this situation. Our second level of concern deals with the variable concern which in this case deals with an individual's attitude and treatment. In our data, we aren't able to consider all of the ages of individuals within the broader population. For example: there are no ages less than 20, meaning we do represent a large amount of our broader population. What if the individuals giving out the surveys were younger and also tended to choose people to approach with a survey that were similar in age to them?  A scenario like this could end up overestimating younger train commuters in the population, which could influence our answers to any of our questions. Specifically, when considering the relationship between `att_end` and `treatment`, this could influence the results of the model because younger individuals may have similar attitudes on immigration.

### Courage

```{r}
#| echo: false
#| fig.cap: Courage
knitr::include_graphics("other/images/Courage.jpg")
```

Once we have the mathematical structure of the model, we use Courage to create a fitted model. In Chapter @sec-four-parameters, we will go into much more detail about the process of created a fitted model. This process will involve transforming the variables as well as selecting the variables to include and to discard. For now, however, we keep things simple.

#### Mathematics

The math for this model is exactly the same as the math for the predictive model in the first part of this chapter, although we change the notation a bit for clarity.

$$ attitude_i = \beta_0 + \beta_1 control_i + \epsilon_i$$

where \n $$control_i \in \{0,1\}$$ \n $$\epsilon_i \sim N(0, \sigma^2)$$

Nothing has changed, except for the meaning of the data items and the interpretations of the parameters. Let's take a look at the parameters of the situation which plays a role alongside the mathematics to create the fitted model.

On the left-hand side we still have the outcome, $y$, however in this case, this is a person's attitude toward immigration after the experiment is complete. $y$ takes on integer values between 3 and 15 inclusive. On the right-hand side, the part contained in the model will consist of the terms $\beta_0$ and $\beta_1$. These two terms stand for the intercept and the control and as before, each term consists of a parameter and a data point. $\beta_0$ is the average attitude toward immigration for treated individuals. $\beta_1$ is the difference between the attitude toward immigration for treated individuals --- those exposed to Spanish-speakers --- and the average attitude toward immigration for control individuals --- those not exposed to Spanish-speakers. These are both our parameters. The $x$'s are our explanatory variables and take the values 1 or 0. In other words, these are binary variables and are mutually exclusive.The last part, $\epsilon$ ("epsilon"), represents the part that is not explained by our model and is called the error term. It is simply the difference between the outcome and our model predictions. In our particular case, this includes all factors that have an influence on someone's attitude toward immigration but are not explained by treatment status. We assume that this error follows a normal distribution with an expected value of 0.

-   Note that the formula applies to everyone in the population, not just the 115 people for whom we have data. The index $i$ does not just go from 1 through 115. It goes from 1 through $N$, where $N$ is the number of individuals in the population. Conceptually, everyone has an `att_end` under treatment and under control.

-   The small $i$'s are an index for the data set. It is equivalent to the "ID" column in our Preceptor Table and simply states that the outcome for person $i$ is explained by the predictor variables ($treatment$ and $control$) for person $i$, along with an error term.

#### Fitted Model

With **Justice** satisfied, we gather our **Courage** and fit the model. Note that, except for the change in variable names, the code is exactly the same as it was above, in our predictive model for age. *Predictive models and causal models use the same math and the same code.* The differences, and they are very important, lie in the interpretation of the results, not in their creation.

```{r}
fit_2 <- stan_glm(att_end ~ treatment, 
                  data = trains, 
                  seed = 45,
                  refresh = 0)

fit_2
```

`att_end` is a measure of person's attitude toward immigration. A higher number means more conservative, i.e., a more exclusionary stance toward immigration into the United States.

Note that once again, since we are using a linear model, we set the `family` argument to "gaussian".

#### Model Checks

Now that we have created our fitted model we need to perform model checks so that we can ensure that the fitted model we created is reasonably accurate. We attempt to the make the model as accurate as possible, however, we can only view whether it is accurate through model checks that we perform. 

Let's decompose the the dependent variable, `att_end` into two parts: the fitted values and the residuals. There are only two possible fitted values, one for the Treated and one for the Control. The residuals, as always, are simply the difference between the outcomes and the fitted values.

```{r}
#| echo: false
outcome <- trains |> 
  ggplot(aes(att_end)) +
    geom_histogram(bins = 50) +
    labs(x = "Attitude toward Immigration",
         y = "Count") 

fitted <- tibble(height = fitted(fit_2)) |> 
  ggplot(aes(height)) +
    geom_bar(width = 0.2) +
    labs(x = "Fitted Values",
         y = NULL) +
    scale_x_continuous(limits = c(3.5, 15),
                       breaks = c(4, 8, 12))

res <- tibble(resids = residuals(fit_2)) |> 
  ggplot(aes(resids)) +
    geom_histogram(bins = 50) +
    labs(x = "Residuals",
         y = NULL) 
  

outcome + fitted + res +
  plot_annotation(title = "Decomposition of Immigration Attitudes into Fitted Values and Residuals")
```

The smaller the spread of the residuals, the better a job the model is doing of explaining the outcomes. The residuals that we have above explains our error that we have within the model. As we can see above, our model did not do a great job of predicting the attitude toward immigration as our fitted values only present two possible values. The credit of this goes towards the residuals as the error within our graph is huge causing for the fitted values to be smaller and less precise.

Another model check that we can and should run is the posterior predictive check. With the help of the posterior predictive check we can run a "fake-data" simulation upon our fitted model that generates a distribution based on the variables at hand. With the help of this check we can view our similar our distribution is when compared with the "fake-data" simulation and the actual data.

```{r}
#| echo: false
pp_check(fit_2, plotfun = "hist", nreps = 3, binwidth = 1)
```

Our graph in the darker blue represents our actual data. As we can see with the lighter blue graph, our fitted model is able to generate a distribution of the data that is similar when compared to the actual data. However, the "fake-data" produces some values for `att_end` which are *impossible.* We know from the survey details that the lowest possible value for `att_end` is 3 and the highest is 15. But, with our "fake-data" simulations, we see several values that are less than 3 and great than 15. This is a flaw in our model. Is it a serious flaw? That is tough to decide. But, for now, we lack the necessary skills to improve the model enough to remove it. In later sections, we will be able to understand new techniques to improve our fitted models that tackles these kinds of flaws.

#### Data Generating Mechanism (DGM)

Recall the Preceptor Table and the units from the beginning. We are concerned with three main ideas from the Preceptor Table: our units (individuals with unique attributes at the train station), our outcome (individual's attitude toward immigration) covariates (the treatment that each individual will receive). Now, let's determine whether our **Data Generating Mechanism** will be linear or logistic. Since our outcome variable `att_end` is a continuous variable since it has a range of possible values, we will use a linear model. With the help of the data generating mechanism that we have created from our fitted model we can fill in the missing values to the Preceptor Tables. We use the data generating mechanism as the last step of **courage** as it allows us to analyze and view our fitted model's data.

Now that we have tested our fitted model through the model checks we can view all of our data with the table that we have below:

```{r}
#| echo: false
gtsummary::tbl_regression(fit_2, intercept = TRUE) %>%
  bold_labels()
```

`(Intercept)` corresponds to 10.0 which is $\beta_0$. As always, R has, behind the scenes, estimated the entire posterior probability distribution for $\beta_0$. But the basic print method for these objects can't show the entire distribution, so it gives us summary numbers: the median and the MAD SD. Speaking roughly, we would expect about 95% of the values in the posterior to be within two MAD SD's of the median. In other words, we are 95% confident that the true, but unknowable, average attitude toward immigration among the Treated in the population to be between 9.2 and 10.8. `treatmentControl` corresponds to $\beta_1$. The `treatmentControl` estimate is -1.5 attitudes younger as the difference between "control" and "treated" has been shown to be younger regarding attitudes for the median values.

Up until now, we have used the Bayesian interpretation of "confidence interval." This is also the intuitive meaning which, outside of academia, is almost universal. There is a truth out there. We don't know, and sometimes can't know, the truth. A confidence interval, and its associated confidence level, tells us how likely the truth is to lie within a specific range. If your boss asks you for a confidence interval, she almost certainly is using this interpretation.

But, in contemporary academic research, the phrase "confidence interval" is usually given a "Frequentist" interpretation. (The biggest divide in statistics is between Bayesians and Frequentist interpretations. The Frequentist approach, also known as "Classical" statistics, has been dominant for 100 years. Its power is fading, which is why this textbook uses the Bayesian approach.) For a Frequentist, a 95% confidence interval means that, if we were to apply the procedure we used in an infinite number of future situations like this, we would expect the true value to fall within the calculated confidence intervals 95% of the time. In academia, a distinction is sometimes made between *confidence intervals* (which use the Frequentist interpretation) and *credible intervals* (which use the Bayesian interpretation). We won't worry about that difference in this *Primer*.

In summary: we model the attitude of individuals at a train station as a linear function of the treatment each individual receives. We find that the individuals that receive any Spanish speaking treatment are about half a standard deviation more in attitude than the attitude for individuals that did not receive Spanish speaking treatment meaning that the individuals that received Spanish speaking treatment ended up being more conservative.

### Temperance

```{r}
#| echo: false
#| fig.cap: Temperance
knitr::include_graphics("other/images/Temperance.jpg")
```

**Courage** gave us the fitted model. With **Temperance** we can create posteriors of the quantities of interest. We should be modest in the claims we make.

#### The Question

Recall the first question with which we began this section:

-   What is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?

#### The Answer


#### Humility





We can use `posterior_epred()` to answer this question. Create a tibble and use it as we have done before:

```{r}
newobs <- tibble(treatment = c("Treated", "Control"))

pe <- posterior_epred(fit_2, newobs) |> 
    as_tibble() |> 
    mutate(ate = `1` - `2`)

pe
```

The posterior probability distribution created with `posterior_epred()` is the same as the one produced by manipulating the parameters directly.

```{r}
pe |> 
  ggplot(aes(x = ate)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100) +
    labs(title = "Posterior for Average Treatment Effect",
         subtitle = "Exposure to Spanish-speakers shifts immigration attitudes rightward",
         x = "Difference in Attitude",
         y = "Probability") +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

<!-- Much more discussion. Explain what a test might be used for in this situation and why we should reject testing. -->

Our second question:

-   What is the largest effect size which still has a 1 in 10 chance of occurring?

Create a tibble which we can pass to `posterior_predict()`. The variables in the tibble which will be passed in as `newdata`. Fortunately, the tibble we created above is just what we need for this question also.

Consider the result of `posterior_predict()` for two people, one treated and one control. Take the difference.

<!-- This goes a bit fast. Slow down. -->

```{r}
pp <- posterior_predict(fit_2, 
                        newdata = newobs) |>
    as_tibble() |>
    mutate(te = `1` - `2`)
  
pp
```

Create a graphic:

```{r}
pp |> 
  ggplot(aes(x = te)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100)  +
    labs(title = "Posterior for Treatment Effect for One Person",
         subtitle = "Causal effects are more variable for indvduals",
         x = "Difference in Attitude",
         y = "Probability") +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

In this case, we are looking at the distribution of the treatment effect for a single individual. This is very different than the *average* treatment effect. In particular, it is much more variable. We are looking at one row in the Preceptor Table. For a single individual, `att_end` can be anywhere from 3 to 15, both under treatment and under control. The causal effect --- the difference between the two potential outcomes can, in theory, be anywhere from -12 to +12. Such extreme values are rare, but not impossible.

The question, however, was interested in the value at the 90th percentile.

```{r}
quantile(pp$te, prob = 0.9)
```

We would not expect a treatment effect of this magnitude to be common, but, at the same time, effects this big and bigger will occur about 10% of the time.

We should always be cautious about our inferences. Our assumptions are never true. We need to be cautious with our inferences when we deal with all of the assumptions that we have made within the chapter regarding validity, stability, representativeness, and model structure. When we take a look at the assumption of validity a possibility may not hold due to when our data was collected, for example if the data was collected in the morning the people that are interviewed may be more grumpy when going to work. When we take a look at the assumption of stability a possibility where it may not hold would be the definition of our party and how the definition of conservative may change over time. When we take a look at the assumption of representativeness a possibility where it may not hold would be with correlation between our "assignment mechanism" and other variables to be true. When we take a look at the assumption of the model structure a possibility where it may not hold would be with the flaw of the fitted model where we conducted a posterior predict check that generated a distribution of values that were less than 3 and above 15 indicating that there are flaws within the model. Our stated inferences almost certainly underestimate the true uncertainty of the world. 

We need to maintain humility when we are making our inferences and decisions. Stay cautious my friends.

<!-- Insert meme! -->



## Summary

Throughout this chapter, we explored relationships between different variables in the `trains` data set. We built two predictive models and two causal models.

Similar to previous chapters, our first task is to always use **Wisdom**. We want to judge how relevant our data is based on the questions we ask. Is it reasonable to consider the data we have (e.g., income and age data from Boston commuters in 2012) are being drawn from the same population as the data we want to have (e.g., income and age data from today for the entire US)? Probably? Recall that these questions will bring up the **assumption of validity** and whether our data will be able to "stack" up with each other.

Our next step is to take a look at **Justice** which helps us decide the best way to represent the models that we will make. A little math won't kill you. Translating that math into code will be through the help of **Courage**. Our primary goal is to generate posterior distributions for the parameters and understand/interpret their meaning. Finally, we end off with **Temperance** that uses our models to answer the questions that we have asked above. Remember it is important to hone in on our questions to the most that we can because the most detailed questions will allow for the more accurate answers to prevail.

*Key Lessons and Commands That Were Talked About:*

-   Create a model using `stan_glm()`.

-   Use `posterior_epred()` to estimate expected values. The **e** in **e**pred stands for **e**xpected.

-   Use `posterior_predict()` to make forecasts for individuals. The variable in predictions is always greater than the variability in expectations because predictions can't pretend that $\epsilon_i$ is zero.

-   Once we have draws from a posterior distribution for our outcome variable --- whether that be an expectation or a prediction --- we can manipulate those draws to answer our question.

*Always Remember the Following:*

-   Always explore your data.

-   Predictive models care little about causality.

-   Predictive models and causal models use the same math and the same code.

-   "When comparing" is a great phrase to start the summary of any non-causal model.

-   Don't confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Be cautious in your use of the posterior.

```{r}
#| echo: false
#| cache: false
#| warning: false
knitr::write_bib(.packages(), "packages.bib")
```
